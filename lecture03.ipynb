{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 웹 크롤링 개관\n",
    "## 웹 크롤링이란?\n",
    "웹 크롤링(web crawling)은 웹을 통하여 서비스되는 문서를 자동으로 수집하여 저장하는 과정을 말한다. 경우에 따라 문서 전체를 저장하기도 하고 문서에서 필요한 부분만을 추출하여 저장하기도 한다. 웹 스크래이핑(web scraping)이라고 부르기도 한다. 웹 크롤링을 위해 만드는 프로그램은 웹 크롤러(web crawler), 웹 스크래이퍼(web scraper), 웹 스파이더(web spider), 웹 로봇(web robot) 등으로 부른다. 구글이나 네이버와 같이 검색 서비스의 제공을 위해 웹 문서의 자동 수집에 이용하는 로봇이 대표적인 웹 크롤러이다.\n",
    "\n",
    "## 웹 크롤링의 원리\n",
    "웹 크롤링의 기본적인 동작 원리는 인간 사용자를 위한 웹 클라이언트인 웹 브라우저(인터넷 익스플로러, 크롬 등)의 동작을 다른 소프트웨어로 흉내내도록 하는 것이다. 크롤링을 위해서는 첫 접근 문서에서 다른 문서로의 링크를 따라 이동하는 기능이 반드시 필요하다. 비교적 간단한 웹 사이트의 경우에는 크롤러가 읽은 HTML 문서를 분석하여 링크들을 추출하고 그 링크를 따라가도록 한다. 구조가 복잡하거나 폼(form) 인터페이스, 연속 스크롤(continuous scroll) 등과 같이 동적으로 콘텐츠가 생성되는 웹 사이트의 경우에는 헤드레스, 혹은 실제 웹 브라우저를 크롤러를 통해 구동해야 하기도 한다. 이러한 웹 사이트의 수집 절차는 일반화하기 어려운 경우가 대부분이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">헤드레스(headless) 브라우저란 화면 상에 표시되는 인터페이스가 없이 메모리 상에서만 동작하는 브라우저를 말한다. 웹 크롤링, 자동 스크린샷 찍기 등을 위해 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한편 다양한 정보 제공을 위한 API 형태의 웹 서비스를 제공되는 사이트의 경우에는 문서와 정보의 수집을 위하여 API를 이용하는 것이 바람직하다. 예를 들어, 트위터에서는 키워드에 의한 문서 탐색, 특정 사용자의 팔로워 얻기 등을 지원하는 API 서비스를 제공한다. 그러므로 트윗을 수집하고자 하는 경우에는 웹 크롤링 방식이 아닌 API 서비스를 이용하는 것이 옳은 접근이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">API(Application Programming Interface)란 운영체제나 특정 프로그래밍 언어에서 사용할 수 있는 기본적인 기능을 라이브러리 등의 형태로 제공하는 것을 말한다. 최근에는 많은 사이트들이 다양한 정보를 제공하는 웹 서비스를 API의 형태로 제공하고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 웹 크롤링의 절차\n",
    "웹 크롤링은 대체로 다음의 순서로 진행한다. 간단한 크롤링의 경우에는 아래의 세 단계를 하나로 합치기도 하고 경우에 따라 두 번째와 세 번째 단계를 하나로 합치기도 한다.\n",
    "\n",
    "### 문서 URL의 수집과 저장\n",
    "문서, 이미지, 동영상, 음악 등 그 형태에 관련 없이 웹에서 접근 가능한 모든 자원은 고유한 URL(Uniform Resource Locator)에 의해 접근이 가능하다. 특정 자원을 가리키는 URL이 바뀌거나 없어지는 경우가 있을 수 있지만 원칙적으로 URL을 알고 있으면 나중에라도 그 자원에 접근할 있다. 그러므로 웹 크롤링의 첫 번째 단계는 웹 문서를 가리키는 URL을 수집하여 저장하는 것이다. 앞서 언급한 바와 같이 간단한 정적 웹 사이트의 경우에는 URL을 얻는 일이 비교적 간단하지만 동적 웹 사이트의 경우에는 URL을 알아내기가 매주 힘든 경우도 있다.\n",
    "\n",
    "### 문서의 수집과 저장\n",
    "이 단계에서는 위의 단계에서 수집한 문서 URL들을 읽어서 문서를 수집하여 저장한다. 동적 웹 사이트의 경우에는 URL을 통한 문서의 수집이 불가능하고 웹 페이지의 특정 요소를 누른다거나 웹 브라우저에서 자바스크립트가 실행되어야 문서에 접근이 가능한 경우도 있다. 단순해 보이는 단계이지만 대규모 크롤링 프로젝트에서는 오류의 대처, 대용량 문서의 효율적인 저장, 작업 이력 관리 등 고려해야 할 사안이 매우 많다.\n",
    "\n",
    "### 문서 단순화와 구조화\n",
    "웹 문서는 대체로 기본적인 텍스트에 구조화, 다양한 장식 등을 위한 태그가 부착된 HTML 형식의 문서이다. 그런데 일반적으로 텍스트 마이닝에서 필요한 것은 태그 등 부가 표식이 모두 제거된 텍스트이므로 이 단계에서 HTML, 자바스크립트 등 순수 텍스트가 아닌 요소를 잘라낸다. 경우에 따라서는 반복적인 머릿말이나 꼬릿말, 광고 등 본문의 요소가 아닌 텍스트를 제외하는 작업을 해야 하기도 한다.\n",
    "\n",
    "## 고려 사항\n",
    "웹 크롤링은 타인 혹은 타조직이 생성한 콘텐츠에 일반적이지 않은 방법으로 접근하는 일이므로 여러 가지 면으로 민감한 작업이다. 현실적으로 볼 때 인간 사용자가 아닌 크롤러의 접근을 환영하는 웹 사이트는 만나기 어렵다. 그러므로 최소한 다음의 사항들은 면밀히 검토하고 정책을 세워야 한다.\n",
    "\n",
    "* 서비스 이용 약관: 개인 웹 사이트를 제외한 대부분의 웹 사이트들은 서비스 이용 약관을 웹 사이트에 게시한다. 이 약관에는 서비스 제공자의 권리와 책임의 한계, 서비스 이용자의 권리 등이 명시되어 있다.\n",
    "* 저작권 보호: 이는 웹 크롤링 자체의 고려 사항이라기보다는 주로 크롤링 이후의 수집된 문서와 자료의 이용에 관련된 것이지만 크롤링을 통해 문서와 자료를 저장하는 행위 역시 저작권 보호에 위배되는 행위일 수 있으므로 주의를 기울여야 한다.\n",
    "* 개인정보 보호: 이 사안 역시 크롤링된 문서의 자료의 활용에 관련된 것인데 최근 특히 많은 주목을 받는 사안이므로 특별히 관심을 두는 것이 좋다.\n",
    "* 크롤링 예절: 앞서 언급한 바와 같이 크롤링에 의한 웹 자원으로의 접근은 인간에 의한 접근과는 매우 다르므로 웹 사이트에 의도하지 않은 영향을 끼칠 가능성이 있다. 그러므로 접근 시간 간격 등 조정 가능한 모든 요소를 면밀히 조정하여 해당 웹 사이트가 정상 상태로 운영되도록 해야 한다.\n",
    "\n",
    "## 웹 크롤링 간단 예제\n",
    "앞 강의에서 자료로 사용한 이청준 작가의 \"벌레 이야기\" 텍스트를 웹에서 크롤하는 간단한 예제를 살펴보자. 목표 URL은 웹 검색을 통해 얻은 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"http://cafe.daum.net/_c21_/bbs_search_read?grpid=MWoV&fldid=IGhr&datanum=1805\"\n",
    "resp = requests.get(url)\n",
    "html = resp.text\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "elem = soup.find(\"xmp\")\n",
    "text = elem.get_text()\n",
    "\n",
    "with open(\"./data/crawling/worm2.txt\", \"w\", encoding = \"utf-8\") as text_file:\n",
    "    print(text, file=text_file )\n",
    "    text_file.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 뉴스 크롤링\n",
    "이제 네이트 뉴스 사이트(<http://news.nate.com>)를 예로 들어, 웹 사이트의 분석에서 크롤러의 작성에 이르까지의 과정을 살펴보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![네이트 뉴스 사이트](figs/nate-news-r.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 웹 사이트 구조 파악과 크롤링 정책 수립\n",
    "### 네이트 뉴스 사이트의 구조\n",
    "네이트 뉴스 사이트는 전형적인 포털 뉴스 서비스 사이트로서 일정한 틀이 갖추어진 상태에서 주기적 혹은 비주기적으로 콘텐츠가 경신된다. 뉴스 사이트의 구조는 대체로 기사의 주제 구분에 의한 면종별 인덱스 페이지가 주어지며 각 인덱스 페이지에는 개별 기사로의 링크가 담긴다. 물론 인덱스 페이지는 페이징을 통해 여러 페이지 분량의 기사의 링크를 서비스한다. 뉴스는 시간의 흐름에 따라 생성되는 콘텐츠이므로 수집 시점이 중요한데 가장 가시적인 기사의 시간 구분은 일자이다. 그러므로 기사들은 일별, 그리고 면종별로 조직되어 있다.\n",
    "\n",
    "### `robots.txt`의 검토\n",
    "웹 사이트에 접근하는 크롤러, 즉 로봇의 접근 범위를 규정하는 `robots.txt`의 내용을 검토하자. 네이트 뉴스의 경우에는 <http://news.nate.com/robots.txt>에 해당 파일이 존재한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "User-agent: Mediapartners-Google\n",
    "Allow: /view/*\n",
    "Allow: /View/*\n",
    "User-agent: *\n",
    "Disallow: /\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 볼 수 있는 `robots.txt`의 내용에 따르면 네이트 뉴스 사이트의 개별 뉴스 문서에 접근이 허용되는 로봇은 Mediapartners-Google이 유일하다. 이 로봇은 구글의 애드센스 기능을 지원하는 로봇이다. 나머지 로봇들은 최상위 디렉토리(`/`)로부터 시작하여 그 하위의 어느 디렉토리에 있는 문서에도 접근이 허용되지 않는다.\n",
    "\n",
    "위의 사항은 강제 사항은 아니다. 또한 구글의 애드센스 로봇이 아닌 다른 로봇이 접근하였을 때에 어떤 동작을 할런지도 실험을 해보기 전에는 알 수 없다. 확실한 것은 이 사이트를 크롤링하기 위해서는 로봇이 아닌 실제 브라우저의 정체성이 필요하다는 것이다.\n",
    "\n",
    "## 사이트 분석\n",
    "이제 크롤러의 구현을 위한 사이트 분석을 수행한다. 앞서 살펴본 바와 같이 이 사이트는 일자별, 면종별로 개별 기사로의 링크를 담고 있으며 페이징이 되는 인덱스 페이지를 가지고 있다. 그러므로 이 인덱스 페이지를 가장 효과적으로 접근하는 방법을 찾아야 한다.\n",
    "\n",
    "### 면종별 인덱스 페이지로의 접근\n",
    "먼저 면종별 인덱스 페이지에 접근하기 위해 경제면 링크를 눌러본다. 경제면 인덱스 페이지에 헤더 부분을 살펴보면 \"최신뉴스\", \"생활 경제\", \"부동산\" 등의 하위 분류가 되어 있음을 알 수 있다. 하위 주제에 무관하게 기사하게 접근할 수 있도록 \"최신뉴스\" 링크를 누른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![면종별 최신 뉴스 인덱스 페이지](figs/nate-news-url-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 그림에서 볼 수 있는 것과 같이 경제면 최신 뉴스 인덱스 페이지의 URL `http://news.nate.com/recent?mid=n0301`이 웹 브라우저의 주소창에 명시적으로 표시된다.\n",
    "\n",
    "다음 단계에서는 날짜 선택과 페이지 적용이 반영된 인덱스 페이지로의 접근 방법을 찾아보자. 앞서 접근한 최신 뉴스 인덱스 페이지의 아랫쪽으로 이동하면 접근 당일을 기준으로 일주일간의 과거 날짜 기사 인덱스 페이지로의 링크와 다른 페이지로의 이동 링크가 있다. 적절한 날짜 링크를 누르고 이어서 하단의 페이지 이동 링크를 눌러 해당 날짜의 3 페이지로 이동해 보자. 그림의 예는 2016년 1월 20일 링크를 누르고 3 페이지로 이동하는 경우이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![면종, 날짜, 페이징 적용 뉴스 인덱스 페이지](figs/nate-news-url-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서와 마찬가지로 웹 브라우저의 주소창에 날짜와 페이징이 반영된 인덱스 페이지의 URL `http://news.nate.com/recent?cate=eco&mid=n0301&type=c&date=20160120&page=3`이 표시된다. 이 URL을 만드는 규칙은 어렵지 않다. `cate` 인자에 `eco`가 주어지면 경제 기사가, `pol`이 주어지면 정치 기사가 선택된다. 다른 면종에 어떤 기호가 사용되는지는 금방 알아볼 수 있다. 그리고 날짜는 YYYYMMDD의 형식으로 `date` 인자에, 페이지 번호는 1 이상의 정수로 `page` 인자에 주어진다.\n",
    "\n",
    "마지막으로 혹시 다른 형태의 인덱스 페이지가 서비스 되는지 살펴보자. 인덱스 페이지의 헤더 바로 밑에 보면 \"제목\", \"제목+내용\", \"포토\"라는 링크가 존재한다. \"제목\"을 눌러보자. 기사의 제목만 간결히 표시되어 한 페이지에 담기는 기사 링크의 양이 많은 형태의 인덱스 페이지가 나타난다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![기사 제목만 표시되는 콘텐츠의 URL](figs/nate-news-url-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그림에서 확인할 수 있는 것처럼 앞서 보인 URL가 거의 같은데 `type=t` 부분만 다르다. 우리가 만들 크롤러에서는 이 URL을 사용하기로 하자.\n",
    "\n",
    "### 페이징 완료 조건\n",
    "앞서 면종과 날짜, 그리고 페이징을 반영하는 인덱스 페이지의 URL을 만드는 규칙을 파악하였다. 페이징은 페이지 번호를 1부터 1씩 증가시켜 적용하는데 총 기사 수를 미리 알 수 없으므로 마지막 페이지를 초과했을 경우 어떻게 동작하는지 페이지 번호를 아주 큰 수로 지정하여 실험을 해보아야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![페이징 완료 페이지](figs/nate-news-paging-stop.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실험 결과 그림에서 볼 수 있는 것처럼 `최신 뉴스가 없습니다`라는 메시지가 표시되는 것을 알 수 있다. 그러므로 페이지 번호를 증가시키며 새로운 URL을 만들어 인덱스 페이지에 접근을 계속하다가 이 메시지가 표시되면 해당 면종, 날짜에 대한 기사 URL 수집을 끝내면 된다.\n",
    "\n",
    "### 개별 기사 링크 패턴\n",
    "이제 웹 브라우저를 이용해 인덱스 페이지의 소스를 들여다 보자. 이 단계에서 해야 할 일은 인덱스 페이지의 소스에서 개별 기사로의 링크들을 어떻게 추출할지를 결정하는 일이다. 인간 이용자라면 인덱스 페이지 화면을 화면을 보면서 링크를 누르겠지만 크롤러는 소스에서 기사로의 링크를 추출하고 그 링크가 표시하는 URL로 개별 기사에 접근해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">엄밀히 말하면 웹 브라우저에서 보여주는 웹 페이지의 소스와 크롤러에서 수집한 웹 페이지의 소스가 같으리라는 보장이 없다. 그러므로 크롤러로 수집한 HTML 파일의 소스를 직접 보는 것이 더 정확하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![인덱스 페이지의 소스](figs/nate-news-news-links.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "소스를 잘 살펴보면 그림에서 보는 바와 같이 `<a href=\"/view/20160120n52546?mid=n0301\">`와 같은 형식으로 개별 기사 링크가 표기되어 있다. 인덱스 페이지에는 기사 링크만 존재하는 것이 아니라 다른 링크들도 포함되어 있다. 따라서 `<a>` 태그로 구성된 링크들 가운데에서 크롤링의 목표인 기사의 링크만 골라낼 수 있어야 한다. 본격적인 크롤러에서는 HTML 문서의 구조적 특성 등을 이용한다. 우리의 경우에는 마침 `<a>` 태그의 `href` 속성으로 주어진 URL이 `/view/`라는 문자열로 시작하는 미완성 URL인 경우가 기사 URL에 해당하므로 이를 이용하여 기사 URL을 골라낼 수 있다.\n",
    "\n",
    "### 인쇄용 기사 URL 생성\n",
    "개별 기사를 수집할 때에는 개별 기사의 페이지가 단순하면 단순할수록 순수 텍스트를 추출하기가 편리하다. 그러한 이유로 웹 사이트를 크롤링할 때에는 가능하면 모바일 페이지, 혹은 인쇄용 페이지 등을 이용하는 것이 좋다. 마침 개별 뉴스 화면 하단에 인쇄 링크가 제공된다. 이를 눌러보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![인쇄용 기사 화면](figs/nate-news-print.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "팝업 창이 뜨면서 인쇄에 적합한 형태이 기사가 표시된다. 팝업 창의 주소창에는 `http://news.nate.com/view/print?aid=20160123n10443`과 같은 형태의 인쇄용 기사의 URL이 나타나 있다. 여기서 핵심은 `aid` 인자에 전달되는 기사 번호이다. 이 번호는 인덱스 페이지에서 수집하는 기사 URL에서 추출이 가능하다.\n",
    "\n",
    "## 기사 URL 수집 스크립트 구현\n",
    "이제 경제면 기사 수집을 위하여 개별 기사의 URL을 수집하는 스크립트를 작성해 보자.\n",
    "\n",
    "### 의사 코드 작성\n",
    "실제 코딩을 하기 전에 가장 먼저 할 일은 의사 코드를 작성하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "1. 기사 수집 대상 날짜를 입력 받는다.\n",
    "2. 출력 파일을 생성한다.\n",
    "3. 페이지 번호를 1에서부터 1씩 증가시키며 아래 동작을 반복한다.\n",
    "    3-1. 인덱스 페이지에 접근하여 HTML을 읽어온다.\n",
    "    3-2. 페이징이 끝나면 반복을 멈춘다.\n",
    "    3-3. 기사 링크에서 URL들을 추출한다.\n",
    "    3-4. 추출한 기사 URL들을 출력 파일에 쓴다.\n",
    "    3-5. 2초 동안 쉰다.\n",
    "5. 출력 파일을 닫는다.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인덱스 페이지에서 개별 기사의 URL을 수집하는 스크립트의 의사 코드는 위와 같이 작성할 수 있다. 이렇게 작성된 의사 코드는 프로그램의 설계를 구체적이고도 명확히 하는 데에 큰 도움이 될 뿐만 아니라 하향식(top down) 설계에 매우 유용하다.\n",
    "\n",
    "### 완성된 스크립트\n",
    "앞서 보인 의사 코드를 파이썬 코드로 구현한 `main()` 함수를 포함한 스크립트의 소스는 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import requests\n",
    "\n",
    "\n",
    "def create_output_file(target_date):\n",
    "    \"\"\"\n",
    "    출력 파일을 생성하고 파일 객체를 돌려준다.\n",
    "    \"\"\"\n",
    "\n",
    "    output_file_name = \"article_urls.\" + target_date + \".txt\"\n",
    "    output_file = open(output_file_name, \"w\", encoding=\"utf-8\")\n",
    "\n",
    "    return output_file\n",
    "\n",
    "\n",
    "def get_html(target_date, page_num):\n",
    "    \"\"\"\n",
    "    주어진 날짜와 페이지 번호에 해당하는 페이지 URL에 접근하여 HTML을\n",
    "    돌려준다.\n",
    "    \"\"\"\n",
    "\n",
    "    user_agent = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) \" + \\\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \" + \\\n",
    "        \"Chrome/37.0.2062.94 Safari/537.36\"\n",
    "    headers = {\"User-Agent\": user_agent}\n",
    "\n",
    "    page_url = \"http://news.nate.com/recent?cate=eco&mid=n0301&type=t&\" + \\\n",
    "        \"date=\" + target_date + \"&page=\" + str(page_num)\n",
    "\n",
    "    response = requests.get(page_url, headers=headers)\n",
    "    html = response.text\n",
    "\n",
    "    return html\n",
    "\n",
    "\n",
    "# page가 \n",
    "def paging_done(html):\n",
    "    \"\"\"\n",
    "    페이징이 완료되었는지를 판단한다.\n",
    "    \"\"\"\n",
    "\n",
    "    done_pat = u\"뉴스가 없습니다.\"\n",
    "\n",
    "    if done_pat in html:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def ext_news_article_urls(html):\n",
    "    \"\"\"\n",
    "    주어진 HTML에서 기사 URL을 추출하여 돌려준다.\n",
    "    \"\"\"\n",
    "\n",
    "    url_frags = re.findall('<a href=\"(.*?)\"', html)\n",
    "    news_article_urls = []\n",
    "\n",
    "    for url_frag in url_frags:\n",
    "        if not url_frag.startswith(\"/view/\"):\n",
    "            continue\n",
    "\n",
    "        url = \"http://news.nate.com\" + url_frag\n",
    "        news_article_urls.append(url)\n",
    "\n",
    "    return news_article_urls\n",
    "\n",
    "\n",
    "def write_news_article_urls(output_file, urls):\n",
    "    \"\"\"\n",
    "    기사 URL들을 출력 파일에 기록한다.\n",
    "    \"\"\"\n",
    "\n",
    "    for url in urls:\n",
    "        print(url, file=output_file)\n",
    "\n",
    "def pause():\n",
    "    \"\"\"\n",
    "    2초 동안 쉰다.\n",
    "    \"\"\"\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "def close_output_file(output_file):\n",
    "    \"\"\"\n",
    "    출력 파일을 닫는다.\n",
    "    \"\"\"\n",
    "\n",
    "    output_file.close()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    주어진 수집 대상 날짜의 네이트 경제 뉴스 기사 URL을 수집한다.\n",
    "    \"\"\"\n",
    "    \n",
    "    target_date = \"20210712\"\n",
    "    output_file = create_output_file(target_date)\n",
    "    page_num = 1\n",
    "\n",
    "    while True:\n",
    "        html = get_html(target_date, page_num)\n",
    "\n",
    "        if paging_done(html):\n",
    "            break\n",
    "\n",
    "        urls = ext_news_article_urls(html)\n",
    "        write_news_article_urls(output_file, urls)\n",
    "        page_num += 1\n",
    "        pause()\n",
    "        # pause()를 쓰는 이유는 사람인척 하려고, 하나씩 누르는 것 처럼\n",
    "\n",
    "    close_output_file(output_file)\n",
    "    \n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `main()`\n",
    "스크립트 소스에서 `main()` 함수의 코드와 앞서 작성한 의사 코드는 거의 일대일로 대응한다. 의사 코드에서 명시하지 않았던 몇 부분이 추가되었을 뿐이다. 잘 작성된 `main()`은 그 자체로도 전체 프로그램의 동작을 나타낼 수 있다.\n",
    "\n",
    "의사 코드로 표현한 `반복`은 파이써 코드에서 `while` 반복문으로 구현되었다. `while` 반복문의 종료 조건을 `True`로 설정하여 무한 반복을 하게 하고 `if` 조건문으로 특정한 조건이 만족할 때 반복을 탈출하도록 하며, 페이지 번호를 증가시키는 것과 같이 상태를 변화시키는 패턴은 반복 처리가 많은 텍스트 마이닝 과제에서 많이 사용된다.\n",
    "\n",
    "### `get_html()`\n",
    "`get_html()` 함수는 우리가 구현하는 웹 크롤러의 핵심 기능을 담당한다. 이 함수의 요체는 Requests 모듈에서 제공하는 `get()` 함수를 이용하는 것이다.\n",
    "\n",
    "이 함수는 크게 세 부분으로 구성되어 있다. 첫 번째 부분에서는 크롤러의 정체성을 설정한다. 웹 표준에서는 이를 사용자 에이전트(User-Agent)라 부른다. 사용자 에이전트는 브라우저명, 운영체제명, 채용된 렌더링 엔진 등의 정보가 문자열로 표현된 것이다. 유효한 사용자 에이전트 문자열은 <http://www.useragentstring.com>과 같은 사이트에서 얻을 수 있다.\n",
    "\n",
    "두 번째 부분에서는 인덱스 페이지에 접근할 수 있는 URL을 생성한다. 앞서 파악한 URL 생성 규칙에 따라 인자로 받은 수집 대상 날짜와 페이지 번호를 이용하여 인덱스 페이지 URL을 구성한다.\n",
    "\n",
    "마지막 부분에서는 requests 모듈의 `get()` 함수를 이용하여 생성된 URL에 접근하고 돌려진 웹 응답에서 HTML을 얻는다. 이 때 앞서 설정한 사용자 에이전트를 이용한다.\n",
    "\n",
    "### `ext_news_article_urls()` \n",
    "`ext_news_article_urls()`는 수집한 HTML에서 개별 기사 URL을 추출한다.\n",
    "\n",
    "이 함수의 핵심은 정규 표현(regular expression)을 지원하는 표준 라이브러리인 re 모듈의 `findall()` 함수를 이용하여 기사 URL을 HTML로부터 추출하는 것이다. 정규 표현에 대해서는 이 강좌에서 자세히 다루지 않는다. 여기서는 `<a href=\"(.*?)\"` 이라는 정규 표현은 위에서 확인한 인덱스 페이지의 소스에서 기사로의 링크에 일치하는 패턴이며, `findall()` 함수는 이 정규 표현에 일치하는 문자열을 대상 문자열에서 모두 찾아서 돌려주는 함수라는 것만 기억하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실행 결과\n",
    "위의 코드가 오류 없이 실행되면 `article_urls.20160823.txt`이라는 이름의 파일이 생성된다. 이 파일을 편집기에서 열어서 보면 다음과 같이 한 줄에 하나의 기사 URL이 기록되어 있다.\n",
    "이렇게 수집한 기사 URL들은 기사 수집 스크립트에서 개별 기사의 접근에 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "http://news.nate.com/view/20160823n28527?mid=n0301\n",
    "http://news.nate.com/view/20160823n28524?mid=n0301\n",
    "http://news.nate.com/view/20160823n28523?mid=n0301\n",
    "http://news.nate.com/view/20160823n28521?mid=n0301\n",
    "http://news.nate.com/view/20160823n28504?mid=n0301\n",
    "http://news.nate.com/view/20160823n28501?mid=n0301\n",
    "http://news.nate.com/view/20160823n05554?mid=n0301\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기사 수집 스크립트 구현\n",
    "### 의사 코드 작성\n",
    "앞서와 같이 먼저 의사 코드를 작성하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "1. 기사 수집 대상 날짜를 입력 받는다.\n",
    "2. 기사 URL 파일을 연다.\n",
    "3. 기사 텍스트 출력 파일을 연다.\n",
    "4. 기사 URL 파일을 한 줄씩 읽으며 더 읽을 내용일 없을 때까지 다음을 반복한다.\n",
    "    4-1. 읽은 줄에서 기사 번호를 추출하여 인쇄용 URL을 만든다.\n",
    "    4-2. 인쇄용 URL로 기사에 접근하여 HTML을 얻는다.\n",
    "    4-3. 얻어온 HTML을 기사 텍스트 출력 파일에 기록한다.\n",
    "    4-4. 3초 동안 쉰다.\n",
    "5. 기사 텍스트 출력 파일을 닫는다.\n",
    "6. URL 목록 파일을 닫는다.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정해야 할 중요한 정책 하나는 출력 파일을 생성하는 방법이다. \n",
    "기사 URL 파일에는 복수 개의 URL이 들어있다. \n",
    "이러한 상황에서 출력 파일을 생성할 때에 개별 기사에 해당하는 복수 개의 파일을 만들 수도 있고 하나의 파일에 여러 개의 기사를 기록할 수도 있다. \n",
    "각각의 방법에 장단점이 있는데 파일의 개수가 늘어남에 따라 하위 디렉토리의 생성 등 복잡한 처리가 뒤따르므로 여기서는 하나의 파일에 여러 개의 기사를 기록하기로 하자. \n",
    "다만 개별 기사의 구분을 위해 정해진 형식의 구분자를 삽입한다.\n",
    "\n",
    "### 완성된 스크립트\n",
    "기사 수집 스크립트는 앞서 구현한 기사 URL 수집 스크립트와 많은 부분을 공유한다. 완성된 스크립트의 소스 코드는 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "\n",
    "\n",
    "def open_url_file(target_date):\n",
    "    \"\"\"\n",
    "    URL 파일을 연다.\n",
    "    \"\"\"\n",
    "\n",
    "    url_file_name = \"article_urls.{}.txt\".format(target_date)\n",
    "    url_file = open(url_file_name, \"r\", encoding=\"utf-8\")\n",
    "\n",
    "    return url_file\n",
    "\n",
    "\n",
    "def create_html_file(target_date):\n",
    "    \"\"\"\n",
    "    출력 파일을 생성한다.\n",
    "    \"\"\"\n",
    "\n",
    "    html_file_name = \"article_htmls.{}.txt\".format(target_date)\n",
    "    html_file = open(html_file_name, \"w\", encoding=\"utf-8\")\n",
    "\n",
    "    return html_file\n",
    "\n",
    "\n",
    "def gen_print_url(url_line):\n",
    "    \"\"\"\n",
    "    주어진 기사 링크 URL로부터 인쇄용 URL을 만들어 돌려준다.\n",
    "    \"\"\"\n",
    "\n",
    "    p = url_line.rfind(\"/\")\n",
    "    q = url_line.rfind(\"?\")\n",
    "    article_id = url_line[p + 1:q]\n",
    "    print_url = \"http://news.nate.com/view/print?aid=\" + article_id\n",
    "\n",
    "    return print_url\n",
    "\n",
    "\n",
    "def get_html(print_url):\n",
    "    \"\"\"\n",
    "    주어진 인쇄용 URL에 접근하여 HTML을 읽어서 돌려준다.\n",
    "    \"\"\"\n",
    "\n",
    "    user_agent = \"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 \" + \\\n",
    "        \"(KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36\"\n",
    "    headers = {\n",
    "        \"User-Agent\": user_agent\n",
    "    }\n",
    "\n",
    "    response = requests.get(print_url, headers=headers)\n",
    "    html = response.text\n",
    "\n",
    "    return html\n",
    "\n",
    "\n",
    "def write_html(output_file, html):\n",
    "    \"\"\"\n",
    "    주어진 HTML 텍스트를 출력 파일에 쓴다.\n",
    "    \"\"\"\n",
    "\n",
    "    print(html, file=output_file)\n",
    "    print(\"@@@@@ ARTICLE DELIMITER @@@@@\", file=output_file)\n",
    "\n",
    "    \n",
    "def pause():\n",
    "    \"\"\"\n",
    "    3초 동안 쉰다.\n",
    "    \"\"\"\n",
    "\n",
    "    time.sleep(3)\n",
    "\n",
    "    \n",
    "def close_output_file(output_file):\n",
    "    \"\"\"\n",
    "    출력 파일을 닫는다.\n",
    "    \"\"\"\n",
    "\n",
    "    output_file.close()\n",
    "\n",
    "    \n",
    "def close_url_file(url_file):\n",
    "    \"\"\"\n",
    "    URL 파일을 닫는다.\n",
    "    \"\"\"\n",
    "\n",
    "    url_file.close()\n",
    "\n",
    "    \n",
    "def main():\n",
    "    \"\"\"\n",
    "    주어진 URL 파일로부터 URL을 읽어서 네이트 뉴스 기사를 수집하여\n",
    "    HTML 출력 파일에 기록한다.\n",
    "    \"\"\"\n",
    "\n",
    "    target_date = \"20210712\"\n",
    "    url_file = open_url_file(target_date)\n",
    "    html_file = create_html_file(target_date)\n",
    "\n",
    "    for line in url_file:\n",
    "        print_url = gen_print_url(line)\n",
    "        html = get_html(print_url)\n",
    "        write_html(html_file, html)\n",
    "\n",
    "    close_output_file(html_file)\n",
    "    close_url_file(url_file)\n",
    "\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 코드에서 눈여겨 볼 만한 부분은 다음과 같다.\n",
    "\n",
    "* `gen_print_url()`: `rfind()` 메소드와 문자열 스플라이싱을 이용하여 기사 번호를 얻는다.\n",
    "* `write_html()`: 기사 파일에 HTML을 기록하고 이어서 구분자 문자열을 기록한다.\n",
    "\n",
    "### 실행\n",
    "위의 스크립트를 실행하면 개별 기사의 HTML 문자열이 연이어 기록된 `article_htmls.20160823.txt` 파일이 생성된다. \n",
    "앞서와 마찬가지로 `main()` 함수를 호출하기 직전의 주석 처리를 조정하면 명령행 인자로 목표 날짜를 지정할 수 있다.\n",
    "\n",
    "## 순수 기사 추출 스크립트 구현\n",
    "### 의사 코드 작성\n",
    "HTML 형식의 기사에서 HTML 태그를 제거하고 기사 제목과 작성 날짜, 시간, 그리고 본문을 구분하여 구조화한 기사를 생성하는 스크립트의 의사 코드는 다음과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "1. 기사 수집 대상 날짜를 입력 받는다.\n",
    "2. HTML 기사 파일을 연다.\n",
    "3. 텍스트 기사 파일을 생성한다.\n",
    "4. 다음의 동작을 반복한다.\n",
    "    4-1. HTML 파일에서 HTML 기사 하나를 읽는다.\n",
    "    4-2. 읽은 HTML 기사가 빈 문자열이면 반복을 멈춘다.\n",
    "    4-3. HTML 기사에서 제목을 추출한다.\n",
    "    4-4. HTML 기사에서 기사 작성 날짜/시간을 추출한다.\n",
    "    4-5. HTML 기사에서 HTML을 제거하고 본문을 추출한다.\n",
    "    4-6. 텍스트 기사 파일에, 제목, 날짜/시간, 본문을 기록한다.\n",
    "5. 텍스트 기사 파일을 닫는다.\n",
    "6. HTML 기사 파일을 닫는다.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 의사 코드는 수집한 HTML 기사의 형식과 구조를 면밀히 파악한 후에야 작성이 가능하다. 비정형 텍스트의 처리에 있어서는 분석 전략은 대상 텍스트의 특성에 크게 의존한다. HTML 기사의 소스를 살펴보자. 먼저 기사의 시작 부분이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![기사 시작 부분 소스](figs/nate-news-src-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 찾아야 할 것은 기사의 제목, 날짜와 시간, 본문을 구분해 줄 수 있는 속성이다. 가장 효과적이면서도 확실한 방법은 HTML 문서를 구문 분석하여 구조적 정보를 이용하거나 XPATH 혹은 CSS 선택자를 이용하는 것이다. 이 강좌에서는 이 방법은 다루지 않는다. 그 대신 비교적 단순한 문자열 단서를 이용한다.\n",
    "\n",
    "위의 소스를 살펴보면 기사 제목은 `<strong>`과 `</strong>`으로 감싸져 있다. 다행인 것은 이 태그가 해당 문서에 오직 이 곳에만 사용되었기 때문에 이 특성은 기사 제목을 추출하는데 매우 효과적이다. 날짜와 시간의 시작과 끝 표식도 마찬가지로 구분된다. 본문의 시작 표식 역시 중의성이 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">몇몇 기사를 더 살펴보면 최종 수정 날짜와 시간이 추가되기도 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![기사 끝 부분 소스](figs/nate-news-src-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기사의 끝 표식은 가장 분명한 문자열을 선택한다. 이제 주어진 문제는 본문에 포함된 HTML 태그 등의 부가 문자열을 제거하는 것이다. 그런데 무조건 제거하기보다는 태그의 정보를 본문의 구성에 적극적으로 사용해야 하는 경우도 있다. 우리의 경우에는 줄바꿈을 표시하는 HTML 태그인 `<br />`을 줄바꿈 문자로 치환하여 기사의 단락 구분을 유지할 수 있다.\n",
    "\n",
    "### 완성된 스크립트\n",
    "위에서 작성한 의사 코드와 기사의 소스 분석을 반영한 스크립트의 코드는 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "ARTICLE_DELIMITER = \"@@@@@ ARTICLE DELIMITER @@@@@\\n\"\n",
    "TITLE_START_PAT = \"<strong>\"\n",
    "TITLE_END_PAT = \"</strong>\"\n",
    "DATE_TIME_START_PAT = u\"기사전송 <em>\"\n",
    "DATE_TIME_END_PAT = \"</em></span>\"\n",
    "BODY_START_PAT = u\"<!-- 기사 내용 -->\"\n",
    "BODY_END_PAT = u\"<!-- //팝업 : 동영상 뉴스 - 뉴스내용 -->\"\n",
    "TIDYUP_START_PAT = \"//<![CDATA[\"\n",
    "\n",
    "\n",
    "def open_html_file(target_date):\n",
    "    \"\"\"\n",
    "    HTML 기사 파일을 열어서 파일 객체를 돌려준다.\n",
    "    \"\"\"\n",
    "\n",
    "    html_file_name = \"article_htmls.{}.txt\".format(target_date)\n",
    "    html_file = open(html_file_name, \"r\", encoding=\"utf-8\")\n",
    "\n",
    "    return html_file\n",
    "\n",
    "\n",
    "def create_text_file(target_date):\n",
    "    \"\"\"\n",
    "    텍스트 기사 파일을 만들어 파일 객체를 돌려준다.\n",
    "    \"\"\"\n",
    "\n",
    "    text_file_name = \"article_texts.{}.txt\".format(target_date)\n",
    "    text_file = open(text_file_name, \"w\", encoding=\"utf-8\")\n",
    "\n",
    "    return text_file\n",
    "\n",
    "\n",
    "def read_html_article(html_file):\n",
    "    \"\"\"\n",
    "    HTML 파일에서 기사 하나를 읽어서 돌려준다.\n",
    "    \"\"\"\n",
    "\n",
    "    lines = []\n",
    "\n",
    "    for line in html_file:\n",
    "        if line.startswith(ARTICLE_DELIMITER):\n",
    "            html_text = \"\".join(lines).strip()\n",
    "            return html_text\n",
    "\n",
    "        lines.append(line)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def ext_title(html_text):\n",
    "    \"\"\"\n",
    "    HTML 기사에서 제목을 추출하여 돌려준다.\n",
    "    \"\"\"\n",
    "\n",
    "    p = html_text.find(TITLE_START_PAT)\n",
    "    q = html_text.find(TITLE_END_PAT)\n",
    "    title = html_text[p + len(TITLE_START_PAT):q]\n",
    "    title = title.strip()\n",
    "\n",
    "    return title\n",
    "\n",
    "\n",
    "def ext_date_time(html_text):\n",
    "    \"\"\"\n",
    "    HTML 기사에서 날짜와 시간을 추출하여 돌려준다.\n",
    "    \"\"\"\n",
    "\n",
    "    p = html_text.find(DATE_TIME_START_PAT)\n",
    "    q = html_text.find(DATE_TIME_END_PAT)\n",
    "    date_time = html_text[p + len(DATE_TIME_START_PAT):q]\n",
    "    date_time = date_time.strip()\n",
    "\n",
    "    return date_time\n",
    "\n",
    "\n",
    "def strip_html(html_body):\n",
    "    \"\"\"\n",
    "    HTML 본문에서 HTML 태그를 제거하고 돌려준다.\n",
    "    \"\"\"\n",
    "\n",
    "    page = BeautifulSoup(html_body, \"html.parser\")\n",
    "    body = page.text\n",
    "\n",
    "    return body\n",
    "\n",
    "\n",
    "def tidyup(body):\n",
    "    \"\"\"\n",
    "    본문에서 필요없는 부분을 자르고 돌려준다.\n",
    "    \"\"\"\n",
    "\n",
    "    p = body.find(TIDYUP_START_PAT)\n",
    "    body = body[:p]\n",
    "    body = body.strip()\n",
    "\n",
    "    return body\n",
    "\n",
    "\n",
    "def ext_body(html_text):\n",
    "    \"\"\"\n",
    "    HTML 기사에서 본문을 추출하여 돌려준다.\n",
    "    \"\"\"\n",
    "\n",
    "    p = html_text.find(BODY_START_PAT)\n",
    "    q = html_text.find(BODY_END_PAT)\n",
    "    html_body = html_text[p + len(BODY_START_PAT):q]\n",
    "    html_body = html_body.replace(\"<br />\", \"\\n\")\n",
    "    html_body = html_body.strip()\n",
    "    body = strip_html(html_body)\n",
    "    body = tidyup(body)\n",
    "\n",
    "    return body\n",
    "\n",
    "\n",
    "def write_article(text_file, title, date_time, body):\n",
    "    \"\"\"\n",
    "    텍스트 파일에 항목이 구분된 기사를 출력한다.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"{}\".format(title), file=text_file)\n",
    "    print(\"{}\".format(date_time), file=text_file)\n",
    "    print(\"{}\".format(body), file=text_file)\n",
    "    print(ARTICLE_DELIMITER, file=text_file, end=\"\")\n",
    "\n",
    "    \n",
    "def main():\n",
    "    \"\"\"\n",
    "    네이트 뉴스 기사 HTML에서 순수 텍스트 기사를 추출한다.\n",
    "    \"\"\"\n",
    "\n",
    "    target_date = \"20210712\"\n",
    "    html_file = open_html_file(target_date)\n",
    "    text_file = create_text_file(target_date)\n",
    "\n",
    "    while True:\n",
    "        html_text = read_html_article(html_file)\n",
    "\n",
    "        if not html_text:\n",
    "            break\n",
    "\n",
    "        title = ext_title(html_text)\n",
    "        date_time = ext_date_time(html_text)\n",
    "        body = ext_body(html_text)\n",
    "        write_article(text_file, title, date_time, body)\n",
    "\n",
    "    html_file.close()\n",
    "    text_file.close()\n",
    "\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 눈에 띄는 함수는 `read_html_article()`이다. 이 함수는 인자로 주어진 파일 객체에서 한 줄씩 문자열을 읽어들여 리스트에 추가하다가 기사 구분자를 만나면 이제까지 그 리스트의 항목들을 하나의 문자열로 만들어 돌려준다. 편리하면서도 흥미로운 것은 이 함수가 `while` 문에서 반복해서 호출될 때마다 한 줄을 읽은 이후의 위치가 기억되어 같은 위치를 계속 읽지 않고 파일의 끝까지 읽기 작업이 진행된다는 것이다. 이는 파일 객체의 특징이다.\n",
    "\n",
    "`ext_title()` 등 기사의 구성 요소를 추출하는 함수들에서는 해당 요소의 시작 패턴과 끝 패턴을 `find()` 메소드를 이용하여 찾아서 문자열 스플라이싱에 의해 해상 요소를 추출해낸다. `ext_body()`에서는 추가로 HTML 태그를 제거하는 `strip_html()` 함수와 그 이후에도 남는 찌꺼기를 제거하는 `tidyup()` 함수를 호출한다.\n",
    "\n",
    "`strip_html()` 함수는 BeautifulSoup 모듈의 HTML 구문 분석 기능을 이용한다. 이의 자세한 동작 원리는 이 강좌의 범위를 넘어서기 때문에 생략한다. `tidyup()` 함수는 특정 패턴 이후의 문자열을 잘라내는 간단한 동작을 한다.\n",
    "\n",
    "### 실행 결과\n",
    "위의 스크립트를 실행하면 앞서 작성된 HTML 기사 파일을 읽어서 기사 항목들이 추출된 `article_texts.20160823.txt` 파일이 생성된다.\n",
    "위에서 설계한 바와 같이 각 기사는 미리 지정한 구분 문자열로 나뉘어지며 각 기사의 첫 번째 행은 기사 제목, 두 번째 행은 기사 작성 날짜와 시간, 이어지는 행들에는 기사 본문이 기록된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 트위터 수집\n",
    "140자 이내의 짧은 포스팅을 올릴 수 있는 마이크로 블로깅 서비스인 트위터는 오늘 대표적인 소셜 네트워크 서비스이다. 사용자들은 자신들의 다양한 일상사를 트위터를 통해 공유하므로 우리는 트윗의 분석을 통해 사람들의 선호, 경향, 유행 등을 분석할 수 있다.\n",
    "\n",
    "생성되는 모든 실시간 트윗, 혹은 과거에 생성된 트윗을 분석하기 위해서는 소셜 데이터 공급 전문 회사인 지닙(GNIP, <https://gnip.com>)에서 제공하는 파이어호스 서비스를 이용해야 한다. 전수 데이터의 수집이 보장되지는 않지만 트위터에서 제공하는 API(https://dev.twitter.com) 통한 데이터 수집도 가능하다. 트위터에서는 여러 가지 형태의 데이터 수집을 지원하는 다수의 API를 제공하는데 우리는 검색 API와 스트리밍 API를 사용해 보자.\n",
    "\n",
    "## Twython 모듈의 설치\n",
    "트위터 API는 HTTP 프로토콜에 기반한다. 그러므로 앞서 뉴스 기사의 수집에 사용한 Requests 모듈을 이용하여 트위터 API에 접근할 수 있다. 그러나 그보다는 사용하기에 편리하며 파이썬스러운(Pythonic) 라이브러리 모듈인 Twython(<https://github.com/ryanmcgrath/twython>)을 사용하는 것이 훨씬 좋은 방법이다. Twython은 아쉽게도 아나콘다 파이썬에서 지원하지 않으므로 다음과 같이 명령행에서 `pip`을 이용하여 설치해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "> pip install twython\n",
    "Collecting twython\n",
    "  Downloading twython-3.4.0.tar.gz\n",
    "Requirement already satisfied (use --upgrade to upgrade): requests>=2.1.0 in /anaconda/lib/python3.5/site-packages (from twython)\n",
    "Collecting requests_oauthlib>=0.4.0 (from twython)\n",
    "  Downloading requests_oauthlib-0.6.2-py2.py3-none-any.whl\n",
    "Collecting oauthlib>=0.6.2 (from requests_oauthlib>=0.4.0->twython)\n",
    "  Downloading oauthlib-1.1.2.tar.gz (111kB)\n",
    "    100% |████████████████████████████████| 112kB 1.6MB/s\n",
    "Building wheels for collected packages: twython, oauthlib\n",
    "  Running setup.py bdist_wheel for twython ... done\n",
    "  Stored in directory: /Users/leekh/Library/Caches/pip/wheels/48/e9/f5/a4c968725948c73f71df51a3c6759425358c1eda2dcf2031f4\n",
    "  Running setup.py bdist_wheel for oauthlib ... done\n",
    "  Stored in directory: /Users/leekh/Library/Caches/pip/wheels/e6/be/43/e4a2ca8cb9c78fbd9b5b14b96cb7a5cc43f36bc11af5dfac5b\n",
    "Successfully built twython oauthlib\n",
    "Installing collected packages: oauthlib, requests-oauthlib, twython\n",
    "Successfully installed oauthlib-1.1.2 requests-oauthlib-0.6.2 twython-3.4.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 트위터 API 사용 권한 획득을 위한 인증\n",
    "트위터 API는 무료로 사용할 수 있으나 사용량(access rate)에 제한이 있으며, 사용 권한을 획득해야만 사용할 수 있다. API 사용 권한은 다음과 같은 절차를 거쳐 얻는다.\n",
    "\n",
    "### 트위터 앱 등록 사이트 접속"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![트위터 앱 등록 사이트 접속](figs/twitter-apps-login.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 트위터 앱 등록 사이트(<https://apps.twitter.com>)에 접속한다. 이미 트위터에 접속한 상태가 아니라면 트위터 서비스 접속을 하게 된다. 참고로 트위터 API를 사용하려면 사용자의 휴대 전화 번호가 프로필에 등록되어 있어야 한다.\n",
    "\n",
    "### 트위터 앱 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![twitter-apps-create](figs/twitter-apps-create.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "트위터 API를 사용하기 위한 권한은 API를 사용하는 애플리케이션별로 받아야 한다. 그러므로 최소한 하나의 애플리케이션을 생성해야 한다.\n",
    "\n",
    "### 트위터 앱 기본 사항 입력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![트위터 앱 기본 사항 입력 상단](figs/twitter-apps-set-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![트위터 앱 기본 사항 입력 하단](figs/twitter-apps-set-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "생성하는 애플리케이션의 기본 사항은 입력한다. 애플리케이션의 이름은 Twitter라는 단어는 포함할 수 없다. 애플리케이션 웹 사이트는 반드시 입력해야 하는데 특별히 입력할 사이트가 없다면 임시로 적절한 값을 입력하고 나중에 수정한다. 합의문에 동의하고 생성 단추를 누른다.\n",
    "\n",
    "### Consumer Key와 Secret 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Consumer Key와 Secret 확인](figs/twitter-apps-consumer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이어서 표시되는 화면에서 `Keys and Access Tokens`를 누르면 애플리케이션의 고유 정보인 Consumer Key (API Key)와 Consumer Secret (API Secret)을 확인할 수 있다.\n",
    "\n",
    "### Access Token과 Access Token Secret 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Access Token과 Acess Token Secret 생성](figs/twitter-apps-access-create.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Access Token과 Acess Token Secret 생성 결과](figs/twitter-apps-access-key.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "같은 화면의 하단에서 Access Token을 생성하는 단추를 눌러 Access Token과 Access Token Secret을 발급받는다. \n",
    "\n",
    "## 검색 API를 이용한 트윗 수집\n",
    "트위터 검색 API(<https://dev.twitter.com/rest/reference/get/search/tweets>)는 트위터 REST API의 일부로 과거 7일간 생성된 트윗의 일부를 대상으로 검색을 수행한다. 이 API는 사용자당 15분 동안 180 번의 이용이 가능한다. 읽기 권한만을 가지는 애플리케이션을 위한 권한을 사용하면 15분 동안 450 번의 이용이 가능하다.\n",
    "\n",
    "Twython 모듈을 이용한 간단한 검색 API 사용례를 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting twython\n",
      "  Downloading twython-3.8.2-py3-none-any.whl (33 kB)\n",
      "Collecting requests-oauthlib>=0.4.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: requests>=2.1.0 in c:\\users\\u0997970\\anaconda3\\lib\\site-packages (from twython) (2.22.0)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\u0997970\\anaconda3\\lib\\site-packages (from requests>=2.1.0->twython) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\u0997970\\anaconda3\\lib\\site-packages (from requests>=2.1.0->twython) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\u0997970\\anaconda3\\lib\\site-packages (from requests>=2.1.0->twython) (1.25.8)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\u0997970\\anaconda3\\lib\\site-packages (from requests>=2.1.0->twython) (2.8)\n",
      "Installing collected packages: oauthlib, requests-oauthlib, twython\n",
      "Successfully installed oauthlib-3.1.1 requests-oauthlib-1.3.0 twython-3.8.2\n"
     ]
    }
   ],
   "source": [
    "!pip install twython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "트위터 검색 API(https://dev.twitter.com/rest/public/search)를 이용하여\n",
    "트윗을 수집한다.\n",
    "\"\"\"\n",
    "\n",
    "from twython import Twython\n",
    "import ujson\n",
    "\n",
    "CONSUMER_KEY = \"YOUR_CONSUMER_KEY\"\n",
    "CONSUMER_SECRET = \"YOUR_CONSUMER_SECRET\"\n",
    "ACCESS_TOKEN = \"YOUR_ACCESS_TOKEN\"\n",
    "ACCESS_TOKEN_SECRET = \"YOUR_ACCESS_TOKEN_SECRET\"\n",
    "\n",
    "twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_TOKEN,\n",
    "                  ACCESS_TOKEN_SECRET)\n",
    "result = twitter.search(q=\"이준석\")\n",
    "\n",
    "for status in result[\"statuses\"]:\n",
    "    print(ujson.dumps(status, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 예제에서는 `search()` 메소드에 키워드 인자 `q`를 지정하여 텍스트 패턴 검색을 수행하였다. 트위터 API 문서에 설명된 대로 다양한 키워드 인자를 추가로 지정하여 복합적인 검색을 할 수도 있다. 검색 결과 데이터는 JSON 형식으로 얻어진다.\n",
    "\n",
    "실행 시점에서 검색 결과가 얼마나 주어지느냐에 따라 스크립트의 실행이 금방 멈추기도 하고 조금 길게 진행되기도 한다. 생성된 결과는 주어진 검색 조건을 만족하는 트윗과 그에 대한 메타 정보가 붙은 JSON 라인들이다.\n",
    "\n",
    "## 스트리밍 API를 이용한 트윗 수집\n",
    "앞서 살펴본 검색 API는 최근 7일간 생성된 트윗의 일부를 대상으로 검색을 수행한다. 그런데 트위터 데이터의 유용성은 실시간 데이터에 접근할 때에 드러난다. 실시간으로 생성되는 트윗을 수집하려면 스트리밍 API(<https://dev.twitter.com/streaming/overview>)를 사용한다. 스트리밍 API는 트위터의 API 서버에 지속적인 접속(persistent connection)을 유지하면서 트윗이 생성될 때마다 지정한 필터링 조건에 맞는 트윗에 맞는 트윗을 골라준다. 다음은 스트리밍 API를 사용하는 예제 스크립트이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "트위터 스트리밍 API(https://dev.twitter.com/streaming/overview)를 이용하여\n",
    "트윗을 수집한다.\n",
    "\"\"\"\n",
    "\n",
    "from twython import TwythonStreamer\n",
    "import ujson\n",
    "\n",
    "CONSUMER_KEY = \"YOUR_CONSUMER_KEY\"\n",
    "CONSUMER_SECRET = \"YOUR_CONSUMER_SECRET\"\n",
    "ACCESS_TOKEN = \"YOUR_ACCESS_TOKEN\"\n",
    "ACCESS_TOKEN_SECRET = \"YOUR_ACCESS_TOKEN_SECRET\"\n",
    "\n",
    "class MyStreamer(TwythonStreamer):\n",
    "    \"\"\"Twitter streamer class.\"\"\"\n",
    "\n",
    "    def on_success(self, data):\n",
    "        \"\"\"스트리밍이 성공했을 때\"\"\"\n",
    "\n",
    "        print(ujson.dumps(data, ensure_ascii=False))\n",
    "\n",
    "    def on_error(self, status_code, data):\n",
    "        \"\"\"스트리밍 오류가 발생했을 때\"\"\"\n",
    "\n",
    "        print(status_code)\n",
    "\n",
    "streamer = MyStreamer(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_TOKEN,\n",
    "                      ACCESS_TOKEN_SECRET)\n",
    "streamer.statuses.filter(track=\"트와이스,레드벨벳,마마무\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 예에서는 `filter()` 메소드의 키워드 인자 `track`에 쉼표로 구분된 복수 개의 키워드를 지정하여 트윗을 필터링한다. 키워드에 공백이 포함되면 두 키워드가 포함된 트윗을 필터링 한다. 즉, AND 조건으로 해석된다. 한 번에 400개의 키워드를 지정할 수 있으며 각 키워드는 60 바이트를 넘을 수 없다. 검색 API와 마찬가지로 필터링 조건을 지정하는 추가의 키워드 인자를 `filter()` 메소드에 지정할 수 있다.\n",
    "\n",
    "스트리밍은 계속 대기하면서 트위터 API 서버로부터의 푸시를 기다리므로 특별한 오류가 발생하지 않는 한 계속 실행된다.\n",
    "결과는 검색에 의한 수집의 경우와 같다.\n",
    "\n",
    "# 스크립트에 의한 웹브라우저 조정\n",
    "데이터 수집 대상이 되는 웹 사이트 가운데에는 목표 데이터를 얻을 수 있는 URL이 명시적으로 주어지 않거나 특정한 요소를 클릭하는 등의 동작이 있어야만 해당 데이터에 접근할 수 있는 경우가 있다. 또한 HTML 텍스트를 웹브라우저가 읽어들인 이후에 자바스크립트가 실행되어 최종 데이터가 생성되는 사이트들도 늘고 있다. 이와 같은 사이트의 데이터 수집을 위해서는 사용자 상호작용과 자바스크립트의 실행을 지원하는 실제 브라우저를 구동해야 한다.\n",
    "\n",
    "파이썬에서 웹브라우저를 구동하는 방법에는 대표적으로 두 가지 방법이 있다. 첫 번째는 Selenium 모듈의 웹드라이버를 이용하여 파이어폭스, 크롬 등의 실제 브라우저나 팬텀제이에스와 같은 헤드리스 브라우저를 구동하는 것이고, 두 번째는 HTML 렌더링 라이브러리인 웹킷을 사용하는 것이다. 이에 대한 자세한 내용은 이 강좌의 범위를 넘어서므로 여기서 그친다.\n",
    "\n",
    "다음 예제 스크립트는 Selenium을 더 편하게 쓸 수 있도록 포장한 Splinter 모듈을 이용하여 파이어폭스 브라우저를 조정하여 네이버 포털에 접속하여 검색을 수행하는 방법을 보인다.\n",
    "이 예제 스크립트를 실행하려면 다음의 준비가 필요하다.\n",
    "\n",
    "* `pip` 명령을 이용하여 Splinter 모듈을 설치해야 한다.\n",
    "* `https://sites.google.com/a/chromium.org/chromedriver/downloads`에서 제공하는 크롬 드라이버를 내려 받아 설치해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "크롬 브라우저를 스크립트로 조종하여 네이버 서비스를 이용한다.\n",
    "\"\"\"\n",
    "\n",
    "import getpass\n",
    "import time\n",
    "import splinter\n",
    "\n",
    "HOME_URL = \"http://www.naver.com\"\n",
    "LOGIN_URL = \"https://nid.naver.com/nidlogin.login\"\n",
    "LOGOUT_URL = \"http://nid.naver.com/nidlogin.logout?returl=http://www.naver.com\"\n",
    "LONG_SLEEP = 4\n",
    "SHORT_SLEEP = 2\n",
    "ID_CSS = \"input#id.int\"\n",
    "PW_CSS = \"input#pw.int\"\n",
    "LOGIN_BTN_CSS = \"input.btn_global\"\n",
    "QUERY_CSS = \"input#query.input_text\"\n",
    "SEARCH_BTN_CSS = \"button#search_btn.sch_smit\"\n",
    "LOGOUT_BTN_CSS = \"span.btn_inr\"\n",
    "\n",
    "\n",
    "def get_naver_user_id_pw():\n",
    "    \"\"\"\n",
    "    네이버 사용자 ID와 암호를 입력받아 돌려준다.\n",
    "    \"\"\"\n",
    "\n",
    "    user_id = input(\"Enter Naver user ID: \")\n",
    "    user_pw = getpass.getpass(\"Enter Naver user password: \")\n",
    "\n",
    "    return user_id, user_pw\n",
    "\n",
    "\n",
    "def login(browser, user_id, user_pw):\n",
    "    \"\"\"\n",
    "    네이버에 로그인한다.\n",
    "    \"\"\"\n",
    "\n",
    "    browser.visit(LOGIN_URL)\n",
    "    pause(long=True)\n",
    "\n",
    "    id_elem = browser.find_by_css(ID_CSS)\n",
    "    id_elem.fill(user_id)\n",
    "    pause()\n",
    "\n",
    "    pw_elem = browser.find_by_css(PW_CSS)\n",
    "    pw_elem.fill(user_pw)\n",
    "    pause()\n",
    "\n",
    "    btn_elem = browser.find_by_css(LOGIN_BTN_CSS)\n",
    "    btn_elem.click()\n",
    "    pause()\n",
    "\n",
    "    \n",
    "def search(browser, query):\n",
    "    \"\"\"\n",
    "    검색어를 입력하여 검색을 실행한다.\n",
    "    \"\"\"\n",
    "\n",
    "    pause()\n",
    "    search_elem = browser.find_by_css(QUERY_CSS)\n",
    "    search_elem.fill(query)\n",
    "    pause()\n",
    "\n",
    "    btn_elem = browser.find_by_css(SEARCH_BTN_CSS)\n",
    "    btn_elem.click()\n",
    "    pause()\n",
    "\n",
    "    \n",
    "def print_html(browser):\n",
    "    \"\"\"\n",
    "    현재 페이지의 HTML을 표시한다.\n",
    "    \"\"\"\n",
    "\n",
    "    print(browser.html)\n",
    "\n",
    "    \n",
    "def logout(browser):\n",
    "    \"\"\"\n",
    "    네이버에서 로그아웃한다.\n",
    "    \"\"\"\n",
    "\n",
    "    browser.visit(HOME_URL)\n",
    "    pause()\n",
    "    browser.visit(LOGOUT_URL)\n",
    "    pause()\n",
    "\n",
    "    \n",
    "def pause(long=False):\n",
    "    \"\"\"\n",
    "    짧게 혹은 길게 쉰다.\n",
    "    \"\"\"\n",
    "\n",
    "    if long:\n",
    "        time.sleep(LONG_SLEEP)\n",
    "    else:\n",
    "        time.sleep(SHORT_SLEEP)\n",
    "\n",
    "        \n",
    "def main():\n",
    "    \"\"\"\n",
    "    파이어폭스 브라우저를 스크립트로 조종하여 네이버 서비스를 이용한다.\n",
    "    \"\"\"\n",
    "\n",
    "    user_id, user_pw = get_naver_user_id_pw()\n",
    "    browser = splinter.Browser(\"chrome\")\n",
    "\n",
    "    login(browser, user_id, user_pw)\n",
    "    search(browser, \"패스트캠퍼스\")\n",
    "    print_html(browser)\n",
    "    logout(browser)\n",
    "\n",
    "    browser.quit()\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 참고\n",
    "* 실용적인 대규모 웹 크롤링을 위해서는 수집 예절의 준수와 유효 시간 내 데이터 수집의 균형을 맞추는 데에 많은 고려를 해야 한다.\n",
    "* 웹 크롤링에서는 예측하기 어려운 일이 많이 발생하므로 미리 대비책을 마련해 두어야 한다.\n",
    "* 컨테이너, 가상 서버, 프록시 서버의 사용 등 다양한 기술적 인프라 지원이 필요하다.\n",
    "* 대규모 크롤링을 위하여 Scrapy(<http://scrapy.org>) 모듈의 사용을 고려하라.\n",
    "* 자바스크립트 실행을 통한 HTML 렌더링을 위해 Splash(<https://github.com/scrapinghub/splash>), PhearJS(<http://phear.io>) 등의 사용을 고려하라.\n",
    "* 다음의 책들을 참조하라.\n",
    "    - 서진수 (2016) 『왕초보! 파이썬 배워 크롤러 DIY 하다!』. 더알음.\n",
    "    - Beltran, Aries (2013) _Getting Started with PhantomJS_. Packt Publishing.\n",
    "    - Collin, Mark (2015) _Mastering Selenium WebDriver_. Packt Publishing.\n",
    "    - Lawson, Richard (2015) _Web Scraping with Python_. Packt Publishing.\n",
    "    - Mitchell, Ryan (2015) _Web Scraping with Python_. O'Reilly Media.\n",
    "    - Russel, Matthew A. (2013) _Mining the Social Web_. 2nd ed. O'Reilly Media."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 연습 문제\n",
    "1. 위에서 보인 뉴스 크롤링과 마찬가지 방법으로 `http://catory.kr`의 게시판 수집을 위한 코드를 작성하라.\n",
    "1. 웹진 사이트 `http://www.inven.co.kr/board/powerbbs.php?come_idx=2097&iskin=webzine`에서 댓글 수집을 위한 코드를 작성하라."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_metadata": {
   "author": "이기황",
   "coursetitle": "텍스트마이닝캠프",
   "courseyear": "2017",
   "date": "2017.04.27",
   "title": "파이썬 텍스트 처리 기법: 웹 크롤링"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
