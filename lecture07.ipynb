{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 군집화\n",
    "## 기본 개념\n",
    "군집화(clustering), 또는 군집 분석(cluster analysis)이란 주어진 객체들을 같은 그룹에 속한 객체들은 최대한 유사하도록(또는 관련이 있도록), 그리고 다른 그룹에 속한 속한 객체들은 최대한 유사하지 않도록(또는 관린이 없도록) 그룹화하는 절차이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![클러스터링의 개념](figs/clustering-concept.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 활용\n",
    "* 이해\n",
    "    - 브라우징을 위해 연관된 문서들을 군집화한다.\n",
    "    - 유사 기능을 갖는 유전자 혹은 단백질을 군집화한다.\n",
    "    - 유사한 가격 변화를 보이는 주식들을 군집화한다.\n",
    "* 요약\n",
    "    - 넓은 지역의 강수량 분포를 군집화한다.\n",
    "    \n",
    "## 종류\n",
    "* 분할 군집화(partitional clustering): 데이터 객체들을 원소가 겹치지 않는 부분 집합으로 나눈다. 비계층적 군집화라고도 부른다.\n",
    "* 계층적 군집화(hierarchical clustering): 계층적 트리에 의해 구성되며 하위 군집을 상위 군집이 포함한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![분할 군집화](figs/partitional-clustering.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![계층적 군집화](figs/hierarchical-clustering.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 유사도의 측정\n",
    "주어진 객체들을 유사한 것들끼리 묶으려면 유사한 정도, 즉 유사도를 측정할 수 있어야 한다. 개념적으로 가장 간단한 유사도 측정 방법은 다음과 같이 측정되는 유클리드 거리(Euclidean distance)를 이용하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![유클리드 거리의 정의](figs/eucl-dist-def.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 정의는 두 점 $A$와 $B$가 각각 두 개의 속성(attribute), 혹은 자질(feature)을 가진 경우, 즉 이 두 점이 이차원 공간에 놓인 경우에 이 두 점의 거리를 정의한 것이다. 이 정의를 확장하여 $n$ 차원 공간에 놓인 두 점 $P$와 $Q$의 좌표가 각각 $P=(p_1,p_2,...,p_n)$, $Q=(q_1,q_2,...q_n)$일 때 이 두 점의 유클리드 거리는 다음과 같이 나타낼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sqrt{(p_1-q_1)^2+(p_2-q_2)^2+\\ldots+(p_n-q_n)^2}=\\sqrt{\\sum_{i=1}^{n}(p_i-q_i)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "유클리드 거리에 의한 유사도 측정을 예를 들어 살펴보자. 한 신용카드 회사에서 고객들을 몇 가지 속성에 따라 군집화하려 한다. 두 고객 $A$와 $B$의 속성이 다음과 같이 주어질 때 이들 간의 거리를 구해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "고객 | 연령 | 현주소 거주 기간 | 거주 형태(1=소유,2=임대,3=기타)\n",
    ":--: | :--: | :--------------: | :-----------------------------:\n",
    "  A  |  23  | 2                | 2\n",
    "  B  |  40  | 10               | 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 보인 유클리드 거리의 식을 이용하여 두 고객의 유사도(거리)를 구하면 다음과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "d(A,B) = \\sqrt{(23-40)^2+(2-10)^2+(2-1)^2} \\approx 18.8\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "군집화를 수행할 때에는 위와 같은 방법으로 두 객체 간의 거리를 구한다. 거리가 가까운 두 객체를 병합하여 생성된 군집들 사이의 거리를 측정할 때에는 군집에 속한 객체들의 속성의 평균값을 이용한다.\n",
    "\n",
    "위의 예제를 파이썬으로 표현하면 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 두 객체의 유클리드 거리 계산\n",
    "\n",
    "import math\n",
    "\n",
    "A = [23, 2, 2]\n",
    "B = [40, 10, 1]\n",
    "deltas = []\n",
    "squares = []\n",
    "\n",
    "for attr_a, attr_b in zip(A, B):\n",
    "    delta = attr_a - attr_b\n",
    "    deltas.append(delta)\n",
    "    \n",
    "for delta in deltas:    \n",
    "    square = delta ** 2\n",
    "    squares.append(square)\n",
    "    \n",
    "sum_square = sum(squares)\n",
    "eucl_dist = math.sqrt(sum_square)\n",
    "\n",
    "print(\"객체 A와 B의 유클리드 거리: {}\".format(eucl_dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문서 군집\n",
    "## 기본 개념\n",
    "문서 군집(document clustering)이란 다수의 문서로 구성된 문서 집합의 문서들을 속성이 유사한 문서들끼리 집단(군집)화하는 절차를 말한다. 이때 **문서의 속성**과 **문서의 유사성**을 어떻게 정의할지가 매우 중요하다. 문서의 속성에는 군집의 목표에 따라 문서의 길이 등 다양한 정보를 반영할 수 있는데 가장 효과적인 것은 개별 문서에 포함된 형태소의 빈도이다. 형태소 빈도는 그대로 사용하기도 할 수도 있지만 보통은 적절한 값으로 정규화(normalize),또는 변환하여 사용한다.\n",
    "\n",
    "문서 군집은 다양하게 응용될 수 있다. 다수의 개별 문서가 포함된 대규모 문서 집합의 속성을 파악하기 힘들 때에 문서 군집을 이용하면 주어진 문서 집합의 개략적인 속성을 요약적으로 이해할 수 있다. 또한 검색 엔진에서 검색 결과를 서비스 이용자에게 돌려줄 때에 비슷한 문서들끼리 묶어서 표시함으로써 검색 결과의 유용성을 증대시킬 수도 있다.\n",
    "\n",
    "한편 기계 학습의 관점에서 볼 때에 문서 군집은 비지도 학습(unsupervised learning)이다. 학습 자료가 주어지지 않은 상태에서 결과를 얻기 때문이다. 이는 다음 강의에서 다룰 문서 분류(document classification)와 대비되는 것이다. 경우에 따라 문서 군집은 지도 학습(supervised learning)인 문서 분류의 사전 준비 작업을 위해 이용되기도 한다.\n",
    "\n",
    "## 어휘 벡터와 문서-어휘 행렬\n",
    "이 강좌의 서두에서 텍스트 마이닝의 중요한 절차 가운데 하나가 \"비정형 텍스트\"를 \"정형 텍스트\"로 바꾸어 표현하는 과정이라고 설명하였다.\n",
    "이를 좀 더 형식적으로 설명하면, 한 텍스트를 이 텍스트의 성질을 나타내는 속성, 또는 자질의 합으로 표현하는 과정이라고 할 수 있다.\n",
    "이 때 유의할 것은 이 속성들이 여러 텍스트의 성질을 비교하기에 적절해야 한다는 것이다.\n",
    "비교가 불가능하다면 사실상 한 텍스트가 다른 텍스트들에 비하여 어떤 특징을 가지고 있는지 알 수 없는 것과도 같기 때문이다. \n",
    "\n",
    "위와 같은 전제 하에 문서 검색, 텍스트 마이닝 등의 분야에서 널리 사용하는 텍스트의 표현 방식은 어휘 벡터를 이용하는 것이다. 어휘 벡터 구성 과정을 간단한 예제로 살펴 보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">어휘 벡터는 용어 벡터(term vector)라고도 부르며 이를 이용하여 텍스트, 혹은 문서를 표현하는 방법을 벡터 공간 모델(vector space model)이라고 부른다. <https://ko.wikipedia.org/wiki/벡터_공간_모델> 참조."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 우리에게 다음과 같이 다섯 개의 개별 문서가 포함된 문서 집합이 주어졌다고 가정하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "D(0) = \"감자 고구마 고구마 고구마 고구마 당근 당근\"\n",
    "D(1) = \"가지 가지 가지 고구마 당근 양파 양파\"\n",
    "D(2) = \"무\"\n",
    "D(3) = \"감자 감자 감자 감자 감자 당근 미역 미역 미역 피망 피망\"\n",
    "D(4) = \"감자 감자 감자 고구마 고구마 고구마 고구마 고구마 고구마 고구마 고구마 고구마 고구마 고구마 고구마 당근 당근 당근 당근 당근 당근\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 문서들의 어휘 빈도를 계수하면 다음과 같은 결과를 얻는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "WC(0) = {\"감자\":1, \"고구마\":4, \"당근\":2}\n",
    "WC(1) = {\"가지\":3, \"고구마\":1, \"당근\":1, \"양파\":2}\n",
    "WC(2) = {\"무\":1}\n",
    "WC(3) = {\"감자\":5, \"당근\":1, \"미역\":3, \"피망\":2}\n",
    "WC(4) = {\"감자\":3, \"고구마\":12, \"당근\":6}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이어서 개별 문서에서 쓰인 어휘를 모두 모으면, 즉 전체 어휘 집합을 만들면 다음을 얻는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "U = {\"가지\", \"감자\", \"고구마\", \"당근\", \"무\", \"미역\", \"양파\", \"피망\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로 전체 어휘 집합을 기준으로 하여 각 문서를 다음과 같이 어휘 벡터로 표현할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "V(0) = {\"가지\":0, \"감자\":1, \"고구마\":4, \"당근\":2, \"무\":0, \"미역\":0, \"양파\":0, \"피망\":0}\n",
    "V(1) = {\"가지\":3, \"감자\":0, \"고구마\":1, \"당근\":1, \"무\":0, \"미역\":0, \"양파\":2, \"피망\":0}\n",
    "V(2) = {\"가지\":0, \"감자\":0, \"고구마\":0, \"당근\":0, \"무\":1, \"미역\":0, \"양파\":0, \"피망\":0}\n",
    "V(3) = {\"가지\":0, \"감자\":5, \"고구마\":0, \"당근\":1, \"무\":0, \"미역\":3, \"양파\":0, \"피망\":2}\n",
    "V(4) = {\"가지\":0, \"감자\":3, \"고구마\":12, \"당근\":6, \"무\":0, \"미역\":0, \"양파\":0, \"피망\":0}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "개별 문서를 표현한 어휘 벡터는 그 문서에 나타나는 어휘만 포함하는 것이 아니라 전체 문서 집합에 나타난 어휘 모두를 포함하고 있다. \n",
    "즉, 개별 문서의 자질은 전체 문서 집합에 공통으로 적용될 수 있는 자질이다.\n",
    "\n",
    "위키피디어(<https://ko.wikipedia.org/wiki/벡터_(물리)>)에 따르면 벡터는 방향과 크기의 의미를 모두 포함하는 표현 도구로 주로 힘이나 자기장, 전기장 등의 물리적 내념을 설명할 때 이용된다.\n",
    "벡터에 포함된 방향 요소들의 개수에 따라 해당 벡터의 차원(dimension)이 결정된다.\n",
    "\n",
    "위의 어휘 벡터들은 8차원의 벡터이며, 방향 요소는 어휘들이며, 그 크기는 단순 어휘 빈도 또는 용어 빈도(TF)로 표현되었다.\n",
    "이 때 크기 요소는 반드시 단순 어휘 빈도로 나타내야 하는 것은 아니다.\n",
    "일반적으로 문서 검색 등에서는 지난 번 강의에서 살펴본 TF-IDF를 사용하는 것이 더 효과적이라고 알려져 있다.\n",
    "방향 요소인 어휘도 마찬가지이다.\n",
    "때로는 문서 집합에 발현한 모든 어휘를 포함하는 것보다 적절히 선택된 어휘들을 사용하는 것이 더 효과적일 때가 많다.\n",
    "아울러 위의 예제에서와 같이 방향 요소로 품사 정보 없는 어휘만 사용하기도 하고 `학교/NC`와 같이 품사를 부착하여 사용하기도 한다.\n",
    "\n",
    "결과적으로 개별 문서를 어휘 벡터로 표현한 문서 집합은 (전체 문서의 수) $\\times$ (전체 어휘의 수) 크기의 행렬로 표현할 수 있다.\n",
    "즉, 다음과 같이 표현된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "DTM = [\n",
    "    {\"가지\":0, \"감자\":1, \"고구마\":4,  \"당근\":2, \"무\":0, \"미역\":0, \"양파\":0, \"피망\":0},\n",
    "    {\"가지\":3, \"감자\":0, \"고구마\":1,  \"당근\":1, \"무\":0, \"미역\":0, \"양파\":2, \"피망\":0},\n",
    "    {\"가지\":0, \"감자\":0, \"고구마\":0,  \"당근\":0, \"무\":1, \"미역\":0, \"양파\":0, \"피망\":0},\n",
    "    {\"가지\":0, \"감자\":5, \"고구마\":0,  \"당근\":1, \"무\":0, \"미역\":3, \"양파\":0, \"피망\":2},\n",
    "    {\"가지\":0, \"감자\":3, \"고구마\":12, \"당근\":6, \"무\":0, \"미역\":0, \"양파\":0, \"피망\":0}\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 행렬을 문서-어휘 행렬(document-term matrix)이라고 부른다.\n",
    "그러므로 비정형 텍스트를 정형 텍스트로 바꾸어 표현하는 과정은 개별 텍스트를 어휘 벡터로 표현하는 과정이며, 결과적으로 전체 텍스트 집합을 문서-어휘 행렬로 표현하는 과정이라고 할 수 있다.\n",
    "\n",
    "## 딕셔너리의 리스트를 이용한 문서-어휘 행렬의 구성\n",
    "이제 앞선 강의에서 살펴본 형태소 분석, 형태소 빈도 계수 등의 기법을 조합하여 문서-어휘 행렬을 구성해 보자.\n",
    "\n",
    "고려해야 할 것은 목표로 하는 문서-어휘 행렬의 자료 구조이다.\n",
    "가장 개념적으로 이해가 쉬운 자료 구조는 어휘 벡터를 딕셔너리로 표현하고 문서-어휘 행렬은 어휘 벡터들의 리스트로 하는 것이다.\n",
    "딕셔너리로 표현한 어휘 벡터의 키는 어휘이고 값은 어휘 빈도가 된다.\n",
    "이를 코드로 구현하면 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 딕셔너리의 리스트를 이용한 문서-어휘 행렬 작성\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from operator import itemgetter\n",
    "import math\n",
    "from konlpy.tag import Hannanum\n",
    "\n",
    "FEATURE_POSES = [\"NC\", \"NQ\"]\n",
    "\n",
    "\n",
    "def morph_anal_documents(documents):\n",
    "    ma_documents = []\n",
    "    hannanum = Hannanum()\n",
    "\n",
    "    for document in documents:\n",
    "        ma_document = get_morph_anal(hannanum, document)\n",
    "        ma_documents.append(ma_document)\n",
    "\n",
    "    return ma_documents\n",
    "\n",
    "\n",
    "def get_morph_anal(hannanum, text):\n",
    "    sent_morph_anals = []\n",
    "    sentences = split_sentences(text)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sent_morph_anal = hannanum.pos(sentence, ntags=22)\n",
    "        sent_morph_anals.append(sent_morph_anal)\n",
    "        \n",
    "    return sent_morph_anals\n",
    "\n",
    "\n",
    "def split_sentences(text):\n",
    "    sentences = re.split(\"(?<=[.?!]) \", text)\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "\n",
    "def get_feature_documents(ma_documents):\n",
    "    feature_documents = []\n",
    "    \n",
    "    for ma_document in ma_documents:\n",
    "        feature_morphs = []\n",
    "        for sent_ma in ma_document:\n",
    "            for morph_lex, morph_cat in sent_ma:\n",
    "                if morph_cat not in FEATURE_POSES:\n",
    "                    continue\n",
    "                    \n",
    "                feature_morphs.append(morph_lex)\n",
    "                \n",
    "        feature_document = \" \".join(feature_morphs)\n",
    "        feature_documents.append(feature_document)\n",
    "        \n",
    "    return feature_documents\n",
    "\n",
    "\n",
    "def build_doc_term_mat(feature_documents):\n",
    "    doc_term_mat = []\n",
    "    morph_counters = get_morph_counters(feature_documents)\n",
    "    all_morphs = get_all_morphs(morph_counters)\n",
    "\n",
    "    for morph_counter in morph_counters:\n",
    "        term_vec = {}\n",
    "\n",
    "        for morph in all_morphs:\n",
    "            term_vec[morph] = morph_counter[morph]\n",
    "\n",
    "        doc_term_mat.append(term_vec)\n",
    "\n",
    "    return doc_term_mat\n",
    "\n",
    "\n",
    "def get_morph_counters(feature_documents):\n",
    "    morph_counters = []\n",
    "\n",
    "    for feature_document in feature_documents:\n",
    "        morph_counter = count_morphs(feature_document)\n",
    "        morph_counters.append(morph_counter)\n",
    "\n",
    "    return morph_counters\n",
    "\n",
    "\n",
    "def count_morphs(feature_document):\n",
    "    morph_counter = Counter()\n",
    "    morph_counter.update(feature_document.split())\n",
    "        \n",
    "    return morph_counter\n",
    "\n",
    "\n",
    "def get_all_morphs(morph_counters):\n",
    "    all_morphs = set()\n",
    "\n",
    "    for morph_counter in morph_counters:\n",
    "        for morph in morph_counter:\n",
    "            all_morphs.add(morph)\n",
    "\n",
    "    return all_morphs\n",
    "\n",
    "\n",
    "def get_pairwise_eucl_dists(doc_term_mat):\n",
    "    eucl_dists = []\n",
    "    term_vec_nums = range(0, len(doc_term_mat))\n",
    "    \n",
    "    for term_vec_num_1, term_vec_num_2 in combinations(term_vec_nums, 2):\n",
    "        eucl_dist = calc_eucl_dist(doc_term_mat[term_vec_num_1], \n",
    "                                   doc_term_mat[term_vec_num_2])\n",
    "        eucl_dists.append((term_vec_num_1, term_vec_num_2, eucl_dist))\n",
    "    \n",
    "    return eucl_dists\n",
    "    \n",
    "    \n",
    "def calc_eucl_dist(vec1, vec2):\n",
    "    deltas = [vec1[e] - vec2[e] for e in vec1]\n",
    "    squares = [d ** 2 for d in deltas]\n",
    "    sum_squres = sum(squares)\n",
    "    dist = math.sqrt(sum_squres)\n",
    "\n",
    "    return dist\n",
    "    \n",
    "\n",
    "def print_eucl_dists(eucl_dists):\n",
    "    for term_vec_num_1, term_vec_num_2, eucl_dist in \\\n",
    "            sorted(eucl_dists, key=itemgetter(2)):\n",
    "        print(\"{}\\t{}\\t{}\".format(term_vec_num_1, term_vec_num_2, eucl_dist))   \n",
    "\n",
    "\n",
    "def main():\n",
    "    documents = [\n",
    "        \"인공지능의 발전과 고용의 미래\",\n",
    "        \"인공지능, 로봇, 빅데이터와 제4차 산업혁명\",\n",
    "        \"인공지능 시대의 법적·윤리적 쟁점\",\n",
    "        \"인공지능(AI) 시대, 교회 공동체 성립요건 연구: 예배와 설교 가능성을 중심으로\",\n",
    "        \"게임 인공지능 연구동향\",\n",
    "        \"파고(인공지능)의 번역 불가능성과 디지털 바벨탑의 붕괴 - 언어의 이종성異種性이 불러올 미래에 대한 가상 시놉시스 -\",\n",
    "        \"인공지능과 심층학습의 발전사\",\n",
    "        \"일반 비디오 게임 플레이 인공지능을 위한 GreedyUCB1기반 몬테카를로 트리 탐색\",\n",
    "        \"포스트휴먼시대 인공지능과 미래 경제 트렌드\",\n",
    "        \"컴퓨터 게임에서의 인공지능 기술\"\n",
    "    ]   \n",
    "    \n",
    "    ma_documents = morph_anal_documents(documents)\n",
    "    feature_documents = get_feature_documents(ma_documents)\n",
    "    doc_term_mat = build_doc_term_mat(feature_documents)\n",
    "    eucl_dists = get_pairwise_eucl_dists(doc_term_mat)\n",
    "    print_eucl_dists(eucl_dists)\n",
    "\n",
    "\n",
    "# 실행\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 스크립트는 딕셔너리의 리스트로 구현한 문서-어휘 행렬의 구성과 문서쌍별 유클리드 거리를 구하여 출력하는 과정을 구현한 것이다. 빈도 계수에는 collections 모듈의 Counter 클래스를 사용한다. \n",
    "\n",
    "문서쌍별 유클리드 거리를 구할 때에 itertools 모듈의 `combinations()` 함수를 사용하는데 어휘 벡터의 쌍을 생성하는 것이 아니라 어휘 벡터 번호의 쌍을 생성하는 방법을 취하여 효율성을 추구하였다. 이는 부수적으로 거리순으로 정렬된 문서쌍별 유클리드 거리를 출력할 때에 문서 번호를 출력하는 효과를 얻게도 한다.\n",
    "\n",
    "## scikit-learn 모듈을 이용한 문서-어휘 행렬 작성\n",
    "문서-어휘 행렬을 작성하여 그 내용을 들여다보면 어휘 벡터의 크기 요소, 즉 어휘 빈도는 대부분이 값이 0이다.\n",
    "이와 같이 대부분의 값이 0으로 채워진 행렬을 희소 행렬(sparse matrix)라 부른다(<https://ko.wikipedia.org/wiki/희소행렬> 참조).\n",
    "희소 행렬은 기억 공간의 낭비가 심하다.\n",
    "따라서 복잡한 벡터와 행렬 연산을 반복적으로 수행해야 하는 기계 학습이나 통계 처리를 위해서는 보다 효율적인 자료 저장 구조가 필요하다.\n",
    "\n",
    "위와 같은 상황에서 가장 널리 사용되는 파이썬 라이브러리 모듈 가운데 하나가 scikit-learn(<http://scikit-learn.org>)이다. \n",
    "scikit-learn은 문서-어휘 행렬과 같은 벡터, 행렬의 효율적인 저장과 계산을 지원하며, 이를 활용한 여러 가지 기계 학습 기능을 제공하는 규모가 큰 라이브러리 모듈이다.\n",
    "\n",
    "이제 이 라이브러리를 이용하여 문서-어휘 행렬을 구성해 본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import scipy as sp\n",
    "from itertools import combinations\n",
    "from operator import itemgetter\n",
    "from konlpy.tag import Hannanum\n",
    "\n",
    "FEATURE_POSES = [\"NC\", \"NQ\"]\n",
    "\n",
    "\n",
    "def morph_anal_documents(documents):\n",
    "    ma_documents = []\n",
    "    hannanum = Hannanum()\n",
    "\n",
    "    for document in documents:\n",
    "        ma_document = get_morph_anal(hannanum, document)\n",
    "        ma_documents.append(ma_document)\n",
    "\n",
    "    return ma_documents\n",
    "\n",
    "\n",
    "def get_morph_anal(hannanum, text):\n",
    "    sent_morph_anals = []\n",
    "    sentences = split_sentences(text)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sent_morph_anal = hannanum.pos(sentence, ntags=22)\n",
    "        sent_morph_anals.append(sent_morph_anal)\n",
    "        \n",
    "    return sent_morph_anals\n",
    "\n",
    "\n",
    "def split_sentences(text):\n",
    "    sentences = re.split(\"(?<=[.?!]) \", text)\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "\n",
    "def get_feature_documents(ma_documents):\n",
    "    feature_documents = []\n",
    "    \n",
    "    for ma_document in ma_documents:\n",
    "        feature_morphs = []\n",
    "        for sent_ma in ma_document:\n",
    "            for morph_lex, morph_cat in sent_ma:\n",
    "                if morph_cat not in FEATURE_POSES:\n",
    "                    continue\n",
    "                    \n",
    "                feature_morphs.append(morph_lex)\n",
    "                \n",
    "        feature_document = \" \".join(feature_morphs)\n",
    "        feature_documents.append(feature_document)\n",
    "        \n",
    "    return feature_documents\n",
    "\n",
    "\n",
    "def build_doc_term_mat(ma_documents):\n",
    "    vectorizer = CountVectorizer(tokenizer=str.split)\n",
    "    doc_term_mat = vectorizer.fit_transform(ma_documents)\n",
    "\n",
    "    return doc_term_mat\n",
    "\n",
    "\n",
    "def get_pairwise_eucl_dists(doc_term_mat):\n",
    "    eucl_dists = []\n",
    "    num_docs, _ = doc_term_mat.shape\n",
    "    word_vec_nums = range(0, num_docs)\n",
    "\n",
    "    for i, j in combinations(word_vec_nums, 2):\n",
    "        eucl_dist = calc_eucl_dist(doc_term_mat.getrow(i),\n",
    "                                   doc_term_mat.getrow(j))\n",
    "        eucl_dists.append((i, j, eucl_dist))\n",
    "\n",
    "    return eucl_dists\n",
    "\n",
    "\n",
    "def calc_eucl_dist(vec1, vec2):\n",
    "    delta = vec1 - vec2\n",
    "    dist = sp.linalg.norm(delta.toarray())\n",
    "\n",
    "    return dist\n",
    "\n",
    "\n",
    "def print_eucl_dists(eucl_dists):\n",
    "    for i, j, eucl_dist in sorted(eucl_dists, key=itemgetter(2)):\n",
    "        print(\"{}\\t{}\\t{}\".format(i, j, eucl_dist))\n",
    "\n",
    "        \n",
    "def main():\n",
    "    documents = [\n",
    "        \"인공지능의 발전과 고용의 미래\",\n",
    "        \"인공지능, 로봇, 빅데이터와 제4차 산업혁명\",\n",
    "        \"인공지능 시대의 법적·윤리적 쟁점\",\n",
    "        \"인공지능(AI) 시대, 교회 공동체 성립요건 연구: 예배와 설교 가능성을 중심으로\",\n",
    "        \"게임 인공지능 연구동향\",\n",
    "        \"파고(인공지능)의 번역 불가능성과 디지털 바벨탑의 붕괴 - 언어의 이종성異種性이 불러올 미래에 대한 가상 시놉시스 -\",\n",
    "        \"인공지능과 심층학습의 발전사\",\n",
    "        \"일반 비디오 게임 플레이 인공지능을 위한 GreedyUCB1기반 몬테카를로 트리 탐색\",\n",
    "        \"포스트휴먼시대 인공지능과 미래 경제 트렌드\",\n",
    "        \"컴퓨터 게임에서의 인공지능 기술\"\n",
    "    ]     \n",
    "    \n",
    "    ma_documents = morph_anal_documents(documents)\n",
    "    feature_documents = get_feature_documents(ma_documents)\n",
    "    doc_term_mat = build_doc_term_mat(feature_documents)\n",
    "    eucl_dists = get_pairwise_eucl_dists(doc_term_mat)\n",
    "    print_eucl_dists(eucl_dists)   \n",
    "\n",
    "\n",
    "# 실행\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 스크립트의 핵심은 sklearn.feature_extraction.text 모듈에서 임포트한 CountVectorizer 클래스를 사용하는 것이다.\n",
    "모듈의 이름에서 짐작할 수 있듯이 이 클래스는 텍스트로부터 자질들을 추출하여 벡터를 구성하는 동작을 한다.\n",
    "클래스의 이름에서 알 수 있는 것은 이 클래스는 자질들의 빈도를 자질로 이용한다는 점이다.\n",
    "\n",
    "이 클래스의 객체의 `fit_transform()` 메소드는 문서 집합을 인자로 받아 문서-어휘 행렬을 생성한다. \n",
    "문서 집합은 각 문서에 해당하는 문자열의 리스트이며, 각 문자열은 보통 공백 문자로 구분된 단어, 혹은 형태소들로 구성된다.\n",
    "이를 위하여 `get_morph_documents()` 함수는 형태소 분석 결과 문서 집합의 각 문서에서 형태소들만 골라서 공백으로 구분된 문자열을 만들어 새로이 문서 집합을 만든다.\n",
    "\n",
    "위의 스크립트를 실행하면 다음과 같은 결과가 화면에 표시된다.\n",
    "\n",
    "앞 부분에 표시된 것은 희소 행렬을 좌표 리스트(COO: Coordinate list)라는 방법을 이용하여 효율적으로 저장한 형태이며, 뒷부분은 이를 희소 행렬 형태로 풀어서 표시한 것이다.\n",
    "\n",
    "문서 집합에서 발현한 모든 단어(형태소)들은 CountVectorizer 객체의 `vocabulary_` 속성으로 참조할 수 있다.\n",
    "\n",
    "# 문서 군집\n",
    "## 문서 군집이란?\n",
    "문서 군집(document clustering)이란 다수의 문서로 구성된 문서 집합의 문서들을 속성이 유사한 문서들끼리 집단(군집)화하는 절차를 말한다. 이때 **문서의 속성**과 **문서의 유사성**을 어떻게 정의할지가 매우 중요하다. 문서의 속성에는 군집의 목표에 따라 문서의 길이 등 다양한 정보를 반영할 수 있는데 앞서 살펴본 어휘 벡터 모델을 이용하는 것이 보통이다. 문서의 유사성은 벡터 간의 유사도 측정에 의하여 판단한다. 물론 군집을 어떠한 방법으로 만들 것인가도 중요하며 군집은 몇 개까지 만드는 것이 적절한지도 판단을 필요로 한다.\n",
    "\n",
    "문서 군집은 다양하게 응용될 수 있다. 다수의 개별 문서가 포함된 대규모 문서 집합의 속성을 파악하기 힘들 때에 문서 군집을 이용하면 주어진 문서 집합의 개략적인 속성을 요약적으로 이해할 수 있다. 또한 검색 엔진에서 검색 결과를 서비스 이용자에게 돌려줄 때에 비슷한 문서들끼리 묶어서 표시함으로써 검색 결과의 유용성을 증대시킬 수도 있다.\n",
    "\n",
    "한편 기계 학습의 관점에서 볼 때에 문서 군집은 비지도 학습(unsupervised learning)이다. 학습 자료가 주어지지 않은 상태에서 결과를 얻기 때문이다. 이는 다음 강의에서 다룰 문서 분류(document classification)와 대비되는 것이다. 경우에 따라 문서 군집은 지도 학습(supervised learning)인 문서 분류의 사전 준비 작업을 위해 이용되기도 한다.\n",
    "\n",
    "## 계층적 군집화\n",
    "계층적 군집화(hierarchicnal clustering)는 개별 자료 항목 하나 하나를 클러스터로 설정한 다음 클러스터 간의 유사성을 판단하여 유사한 클러스터들끼리 합병 동작을 모든 클러스터들이 하나의 클러스로 합쳐질 때까지 반복 수행하는 상향식(bottom up) 방식의 군집 방식이다.\n",
    "\n",
    "계층적 군집화에는 자료 간의 거리, 혹은 유사도를 측정하는 방법, 거리를 최적화하는 방법, 군집 간의 연결(linkage)을 만드는 방법 등에 따라 여러 가지 기법이 존재한다.\n",
    "\n",
    "다음은 자료 항목간의 거리를 유클리드 거리로 정의하고 군집 간의 거리는 두 군집에 속한 속성들의 값이 평균으로부터 떨어진 정도인 편차 제곱을 모두 합하여 구하는 워드(ward) 기법을 사용하는 계층적 군집화의 사례이다.\n",
    "군집 분석 대상 문서 집합은 노무현 전직 대통령의 연설문으로 구성하였다.\n",
    "연설문은 총 780건이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 대통령 연설문의 계층적 군집화\n",
    "\n",
    "import sys\n",
    "import ujson\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "INPUT_FILE_NAME = \"../data/speeches/nmh_speeches.ma.txt\"\n",
    "OUTPUT_FILE_NAME = \"../data/speeches/nmh_speeches.dendro.png\"\n",
    "FEATURE_POSES = [\"NC\", \"NQ\"]\n",
    "MA_KEY = \"body_ma\"\n",
    "\n",
    "\n",
    "if sys.platform.startswith(\"win\"):\n",
    "    matplotlib.rc(\"font\", family=\"Malgun Gothic\")\n",
    "elif sys.platform.startswith(\"darwin\"):\n",
    "    matplotlib.rc(\"font\", family=\"AppleGothic\")\n",
    "    \n",
    "\n",
    "def read_documents():\n",
    "    documents = []\n",
    "\n",
    "    with open(INPUT_FILE_NAME, \"r\", encoding=\"utf-8\") as input_file:\n",
    "        for line in input_file:\n",
    "            morphs = []\n",
    "            json_obj = ujson.loads(line)\n",
    "\n",
    "            for sent_anal in json_obj[MA_KEY]:\n",
    "                for morph_lex, morph_cat in sent_anal:\n",
    "                    if morph_cat not in FEATURE_POSES:\n",
    "                        continue\n",
    "\n",
    "                    morphs.append(morph_lex)\n",
    "\n",
    "            document = \" \".join(morphs)\n",
    "            documents.append(document)\n",
    "\n",
    "    return documents\n",
    "\n",
    "\n",
    "def read_president_fields():\n",
    "    president_fields = []\n",
    "\n",
    "    with open(INPUT_FILE_NAME, \"r\", encoding=\"utf-8\") as input_file:\n",
    "        for line in input_file:\n",
    "            json_obj = ujson.loads(line)\n",
    "            president = json_obj[\"president\"]\n",
    "            field = json_obj[\"field\"]\n",
    "            president_fields.append(president + \"-\" + field)\n",
    "\n",
    "    return president_fields\n",
    "\n",
    "\n",
    "def build_doc_term_mat(documents):\n",
    "    vectorizer = TfidfVectorizer(tokenizer=str.split)\n",
    "    doc_term_mat = vectorizer.fit_transform(documents)\n",
    "\n",
    "    return doc_term_mat\n",
    "\n",
    "\n",
    "def get_hier_clusters(doc_term_mat):\n",
    "    clusters = linkage(doc_term_mat.toarray(), \"ward\")\n",
    "\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def plot_linkage(clusters, president_fields):\n",
    "    plt.figure(figsize=(40, 80))\n",
    "    dendrogram(clusters, orientation=\"right\", labels=president_fields,\n",
    "               leaf_font_size=5)\n",
    "    plt.savefig(OUTPUT_FILE_NAME, dpi=200)\n",
    "    # 덴드로그램을 화면에 직접 표시라면 아래의 주석을 해제한다.\n",
    "    # plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    \n",
    "def main():\n",
    "    documents = read_documents()\n",
    "    president_fields = read_president_fields()\n",
    "    doc_term_mat = build_doc_term_mat(documents)\n",
    "    clusters = get_hier_clusters(doc_term_mat)\n",
    "    plot_linkage(clusters, president_fields)\n",
    "\n",
    "    \n",
    "# 실행\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 스크립트를 부분부분 살펴보자.\n",
    "\n",
    "* `main()` 함수는 `read_speeches()`, `read_president_fields()`, `build_doc_term_mat()`, `get_hier_clusters()`, 그리고 `plot_linkage()` 함수를 호출한다.\n",
    "* `read_speeaches()` 함수는 대통령 연설문 형태소 분석 파일을 열어서 형태소들로 구성된 문자열을 원소로 하는 연설문 문서 집합을 읽어서 돌려준다. \n",
    "이 때 일반명사, 고유명사, 그리고 어근을 읽어들이며 나머지 품사의 형태소들은 건너뛴다.\n",
    "* `read_president_fields()` 함수는 대통령 연설문 형태소 분석 파일에서 개별 연설물에 해당하는 대통령 이름과 연설 분야를 읽어서 줄표로 연결하여 하나의 문자열로 만들어 리스트로 모아서 돌려준다. \n",
    "이는 군집 분석 결과를 덴드로그램으로 시각화할 때 각 노드에 붙이는 레이블로 사용한다.\n",
    "* `build_doc_term_mat()` 함수에서는 문서-어휘 행렬를 생성한다.\n",
    "앞서 실험 예제와 다른 점은 단순 용어 빈도 자질로 사용하는 CountVectorizor 클래스가 아닌 TF-IDF 정규화가 이루어진 TfidfVectorizer 클래스를 사용한다는 점이다.\n",
    "* `get_hier_clusters()` 함수는 계층적 군집 분석을 수행하는 함수이다.\n",
    "이를 위해 scipy.cluster.hierarchy 모듈에서 제공하는 `linkage()` 함수를 사용한다.\n",
    "* `plot_linkage()` 함수에서는 계층적 군집 분석 결과를 시각화한다.\n",
    "이를 위해서 scipy.cluster.hierarchy 모듈의 `dendrogram()` 함수를 사용하며 그림의 생성과 저장에는 matploblib 모듈을 사용한다.\n",
    "\n",
    "위 스크립트를 실행하면 `nmh_speeches.dendro.png`라는 출력 그래픽 파일이 만들어진다.\n",
    "그림에 따르면 노무현 대통령의 연설문은 크게 네 개의 클러스터로 구성되어 있다.\n",
    "군집화 결과를 해석하기 위해서는 군집 문서별 키워드 분석 등 추가 분석이 많이 필요할 수 있다.\n",
    "경우에 따라서는 분석 대상 문서의 성질에 따라 해석 자체가 힘든 경우도 발생한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![덴드로그램](figs/nmh_speeches.dendro.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 비계층적 군집화\n",
    "비계층적 군집화(non-hierarchicnal clustering)는 자료 항목들을 주어진 갯수의 군집들로 분할(partition)하는 군집화 방법으로 처리 속도가 빨라 많은 분야에서 사용하고 있다. 가장 널리 사용되는 기법은 K-평균(k-means)이다. \n",
    "\n",
    "K-평균 기법은 같은 군집에 속한 자료 항목들은 서로 거리가 \"가깝다\"라는 간단한 가정에서 시작한다. 각각의 군집에는 **중심(centroid)**을 하나씩 설정하고 이로부터 각각의 자료와의 거리는 비용(cost)으로 정의한다. 결국 K-평균은 이와 같이 정의한 비용을 최소화하는 군집을 찾는 알고리즘이다.\n",
    "\n",
    "앞서 계층적 군집화의 대상이 된 대통령 연설문 집합을 대상으로 K-평균 군집화 실험을 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ujson\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "INPUT_FILE_NAME = \"../data/speeches/nmh_speeches.ma.txt\"\n",
    "FEATURE_POSES = [\"NC\", \"NQ\"]\n",
    "MA_KEY = \"body_ma\"\n",
    "NUM_CLUSTERS = 5\n",
    "\n",
    "\n",
    "def read_documents():\n",
    "    documents = []\n",
    "\n",
    "    with open(INPUT_FILE_NAME, \"r\", encoding=\"utf-8\") as input_file:\n",
    "        for line in input_file:\n",
    "            morphs = []\n",
    "            json_obj = ujson.loads(line)\n",
    "\n",
    "            for sent_anal in json_obj[MA_KEY]:\n",
    "                for morph_lex, morph_cat in sent_anal:\n",
    "                    if morph_cat not in FEATURE_POSES:\n",
    "                        continue\n",
    "\n",
    "                    morphs.append(morph_lex)\n",
    "\n",
    "            document = \" \".join(morphs)\n",
    "            documents.append(document)\n",
    "\n",
    "    return documents\n",
    "\n",
    "\n",
    "def build_doc_term_mat(documents):\n",
    "    vectorizer = TfidfVectorizer(tokenizer=str.split)\n",
    "    doc_term_mat = vectorizer.fit_transform(documents)\n",
    "    words = vectorizer.get_feature_names()\n",
    "\n",
    "    return doc_term_mat, words\n",
    "\n",
    "\n",
    "def get_flat_clusters(doc_term_mat):\n",
    "    km = KMeans(n_clusters=NUM_CLUSTERS, init=\"k-means++\", verbose=1)\n",
    "    km.fit(doc_term_mat)\n",
    "\n",
    "    return km\n",
    "\n",
    "\n",
    "def print_centroid_words(model, words):\n",
    "    print(\"군집별 중심 어휘\")\n",
    "    print()\n",
    "\n",
    "    ordered_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "    for cluster_num in range(NUM_CLUSTERS):\n",
    "        center_word_nums = [word_num for word_num \n",
    "                            in ordered_centroids[cluster_num, :20]]\n",
    "        center_words = [words[word_num] for word_num in center_word_nums]\n",
    "        print(\"군집 {}: {}\".format(cluster_num, \", \".join(center_words)))\n",
    "        print()\n",
    "\n",
    "    print()\n",
    "    \n",
    "\n",
    "def main():\n",
    "    documents = read_documents()\n",
    "    doc_term_mat, words = build_doc_term_mat(documents)\n",
    "    model = get_flat_clusters(doc_term_mat)\n",
    "    print_centroid_words(model, words)\n",
    "\n",
    "\n",
    "# 실행\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 비계층적 군집화 스크립트는 앞서 구현한 계층적 군집화 스크립트와 많은 부분이 겹친다.\n",
    "\n",
    "* `build_doc_term_mat()` 함수는 앞서와 마찬가지로 TfidfVectorizer 클래스를 이용하여 문서-어휘 행렬을 생성한다. \n",
    "차이점은 중심 어휘 출력을 위해 사용할 어휘 목록을 벡터라이저 객체로부터 `get_features_names()` 메소드를 이용하여 얻어서 문서-어휘 행렬과 함께 돌려준다는 것이다.\n",
    "* `get_flat_clusters()` 함수는 ​sklearn.cluster 모듈에서 제공하는 KMeans 클래스를 이용하여 K-평균 비계층적 군집화를 수행한다.\n",
    "설정하는 인자들은 다음과 같다.\n",
    "    - `n_clusters`: 목표로 하는 군집의 갯수를 설정한다. 군집의 갯수는 실험에 의해 설정하는 것이 보통이다.\n",
    "    - `init`: 군집의 초기 상태를 설정하는 방법을 지정한다. `k-means++`는 효과적인 군집화에 적합한 초기화 방법으로 알려져 있다. 다른 값으로는 `random`을 지정할 수 있다.\n",
    "    - `verbose`: 군집을 수행할 때에 화면에 진행 상황을 표시하도록 하려면 `1`을 지정하고 그렇지 않게 하려면 `0`을 지정한다.\n",
    "* `print_centroid_words()` 함수는 각 군집별로 중심에 근접한 어휘 20개까지를 출력한다. 각 군집의 중심은 클러스터링 결과 모델 객체의 `cluster_centers_` 속성에 들어 있다. 이는 2차원 배열인데, `argsort()` 메소드를 이용하여 정렬한 다음 일부만 추출하여 각 어휘의 번호를 얻고, 얻은 번호로 실제 어휘를 찾아서 출력한다. \n",
    "\n",
    "위의 스크립트를 실행하면 여러 번의 최적화 연산을 거쳐서 최종 결과가 도출되어 다음과 같은 결과가 출력된다.\n",
    "위 스크립트에서는 보이지 않았지만 클러스터링 모델 객체의 `labels_` 속성을 이용하면 문서 집합 내의 문서별로 부여된 군집 번호를 얻을 수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_metadata": {
   "author": "이기황",
   "coursetitle": "텍스트마이닝캠프",
   "courseyear": "2017",
   "date": "2017.06.01",
   "title": "텍스트 군집화"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
