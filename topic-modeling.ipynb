{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토픽 모델링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토픽 모델링의 절차\n",
    "### 토픽 모델링 간단 예제\n",
    "\n",
    "우리에게 다음과 같은 문장들이 주었다고 가정하자.\n",
    "\n",
    "1. 나는 **물고기**와 **채소**를 **먹는다**.\n",
    "1. _물고기_는 _반려동물_이다.\n",
    "1. _고양이_가 **물고기**를 **먹는다**.\n",
    "\n",
    "위의 문장들에서 강조된 단어들을 두 개의 토픽에 할당할 수 있다. 즉, 굵은 글씨로 강조된 단어들은 \"음식\" 토픽, 기울인 글씨로 강조된 단어들은 \"반려동물\" 토픽에 할당할 수 있다.\n",
    "\n",
    "이와 같이 단어 수준에서 토픽을 정의하면 다음과 같은 이점이 있다.\n",
    "\n",
    "1. 문장별 토픽의 분포를 어휘 빈도로 나타낼 수 있다. 위의 예에서 문장 1은 \"음식\" 토픽 100%로 구성되어 있으며, 문장 2는 \"반려동물\" 토픽 100%로 구성되어 있다. 한편 문장 3은 \"반려동물\" 토픽 33%, \"음식\" 토픽 67%로 구성되어 있다.\n",
    "1. 개별 토픽의 내용을 해당 토픽을 구성하는 단어들의 비율을 이용해 계량적으로 나타낼 수 있다. 위의 예에서 \"음식\" 토픽은 '먹는다' 40%, '물고기' 40%, '채소 '20%로 구성된다고 보일 수 있다.\n",
    "\n",
    "### 단순화한 토픽 모델링의 절차\n",
    "이상과 같은 토픽 모델링의 절차를 간략히 설명하기 위해 다음과 같이 두 개의 문서가 주어졌다고 가정하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| T | 문서 X | T | 문서 Y |\n",
    "|:-:|:------:|:-:|:------:|\n",
    "|   | 물고기 |   | 물고기 |\n",
    "|   | 물고기 |   | 물고기 |\n",
    "|   | 먹는다 |   | 우유   |\n",
    "|   | 먹는다 |   | 고양이 |\n",
    "|   | 채소   |   | 고양이 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1단계\n",
    "목표로 하는 토픽의 개수를 지정한다. 보통 몇 번의 시행을 통해 적절한 토픽의 개수를 선택하는데 연구자의 주관적 선택에 의해 결정되는 경우가 많다.\n",
    "\n",
    "#### 2단계\n",
    "문서 집합에 존재하는 모든 단어들에 임시로 토픽을 할당한다. 최초 토픽 할당은 LDA의 경우 디리슐레 분포에 의하여 이루어진다. 같은 단어라 하더라도 서로 다른 여러 토픽에 할당될 수 있다.\n",
    "\n",
    "#### 3단계 (반복)\n",
    "문서 집합에 모든 단어에 대하여 다음의 두 가지 기준에 따라 토픽 할당을 업데이트한다.\n",
    "\n",
    "* 점검 대상 단어가 각각의 토픽들에서 차지하는 비율이 얼마나 되는가?\n",
    "* 점검 대상 단어가 속한 문서에서 각각의 토픽들이 차지하는 비율이 얼마나 되는가?\n",
    "\n",
    "즉, 현재 단어가 현재 토픽에서 차지하는 비율이 크고, 현재의 토픽이 현재의 문서에서 차지하는 비율이 크면 현재의 토픽 할당이 강화되고 그렇지 않으면 다른 토픽에 할당한다. 이 두 기준이 어떻게 적용되는지 아래의 예를 보자. 아래 예는 문서 Y에서 단어 '물고기'의 토픽 할당을 점검하는 단계이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| T | 문서 X | T | 문서 Y |\n",
    "|:-:|:------:|:-:|:------:|\n",
    "| F | 물고기 | ? | 물고기 |\n",
    "| F | 물고기 | F | 물고기 |\n",
    "| F | 먹는다 | F | 우유   |\n",
    "| F | 먹는다 | P | 고양이 |\n",
    "| F | 채소   | P | 고양이 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**점검 대상 단어가 각각의 토픽들에서 차지하는 비율이 얼마나 되는가?** 두 문서에 모두 속한 단어 '물고기'는 토픽 F의 거의 반을 차지하지만 토픽 P에는 전혀 속하지 않고 있다. 그러므로 임의로 취한 단어 '물고기'는 토픽 F에 속할 가능성이 높다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| T | 문서 X | T | 문서 Y |\n",
    "|:-:|:------:|:-:|:------:|\n",
    "| **F** | **물고기** | ? | 물고기 |\n",
    "| **F** | **물고기** | **F** | **물고기** |\n",
    "| F | 먹는다 | F | 우유   |\n",
    "| F | 먹는다 | P | 고양이 |\n",
    "| F | 채소   | P | 고양이 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**점검 대상 단어가 속한 문서에서 각각의 토픽들이 차지하는 비율이 얼마나 되는가?** 문서 Y에 속한 단어들이 50대 50 비율로 토픽 F와 토픽 P에 할당되어 있으므로 점검 대상 단어 '물고기'가 두 토픽에 할당될 가능성은 동일하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| T | 문서 X | T | 문서 Y |\n",
    "|:-:|:------:|:-:|:------:|\n",
    "| F | 물고기 | ? | 물고기 |\n",
    "| F | 물고기 | **F** | **물고기** |\n",
    "| F | 먹는다 | **F** | **우유**   |\n",
    "| F | 먹는다 | _P_ | _고양이_ |\n",
    "| F | 채소   | _P_ | _고양이_ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 기준을 종합해 볼 때 문서 Y에 속한 점검 대상 단어 '물고기'는 토픽 F에 할당하는 것이 적합하다는 결론을 내릴 수 있다.\n",
    "\n",
    "LDA를 이용하면 위와 같은 토픽 할당 점검을 모든 문서에 속한 모든 단어들에 대하여 반복적으로 수행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">위의 설명은 <https://algobeans.com/2015/06/21/laymans-explanation-of-topic-modeling-with-lda-2>에서 제공하는 튜토리얼 문서를 우리말로 요약하여 옮긴 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토픽 모델링의 수리적 이해"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 보인 토픽 모델링 절차를 간단히 수리적으로 표현해 보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "P(Z|W,D) = \n",
    "\\frac{\\text{토픽 $Z$에 속한 단어 $W$의 수}+\\beta_w}{\\text{토픽 $Z$에 속한 단어 수 총합}+\\beta} \\times (\\text{토픽 $Z$에 속한 문서 $D$ 소속 단어 수} + \\alpha)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 곱셈의 첫 번째 항은 단어 $W$가 토픽 $Z$에서의 비율이 얼마나 되는가를 측정하는 것이고 두 번째 항은 단어 $W$가 속한 문서 $D$에서 토픽 $Z$가 차지하는 비율이 얼마나 되는가를 측정하는 것이다. $\\alpha$와 $\\beta$는 하이퍼파라미터라고 부르는 계수로 모델 할당을 다소 느슨하게 만들어 준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">위의 식은 <https://tedunderwood.com/2012/04/07/topic-modeling-made-just-simple-enough/>에서 가져온 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한편 Blei (2012)에서는 다음과 같은 토픽 모델링 식을 제시한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(\\beta_{1:K}, \\theta_{1:D}, z_{1:d}, w_{1:D})\\\\\\\\\n",
    "=\\Pi_{i=1}^K P(\\beta_i) \\Pi_{d=1}^D P(\\theta_d) \n",
    "\\big(\\Pi_{n=1}^N P(z_{d,n} | \\theta_d) P(w_{d,n} | \\beta_{1:K}, z_{d,n}) \\big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{aligned} \\beta_{1:K} & \\text{토픽들}\\\\\\\\ \\theta_{1:D} & \\text{문서별 토픽 비율}\\\\\\\\ z_{1:d} & \\text{문서별 토픽 할당}\\\\\\\\ w_{1:d} & \\text{문서별 단어}\\\\\\\\           \\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 식을 살펴보면 토픽 할당 $z_{d,n}$은 문서별 토픽 비율인 $\\theta_d$에 의존하며, 관찰된 단어 $w_{d,n}$은 토픽 할당 $z_{d,n}$과 전체 토픽 $\\beta_{1:K}$에 의존한다. 이 의존성에 근거하여 변수들의 결합 확률 분포가 디리슐레 분포라는 특정한 분포에 따른다는 가정을 하게 된다. 여기서 직접 관찰 가능한 변수는 $w_{1:d}$밖에 없는데, 이를 조건으로로 삼아 다음과 같은 조건부 확률을 구하는 식을 만든다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(\\beta_{1:K}, \\theta_{1:D}, z_{1:d} | w_{1:D})\\\\\\\\\n",
    "= \\frac{P(\\beta_{1:K}, \\theta_{1:D}, z_{1:d}, w_{1:D})}{P(w_{1:D})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 확률을 계산하려면 가능한 모든 토픽 할당 경우의 수를 생성하여야 한다. 그런데 실제로는 그럴 수 없기 때문에 반복적으로 표본 추출을 하여 확률값을 추정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">위에서 사용하는 표본 추출 기법은 깁스 표본(Gibbs sampling) 기법이다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
