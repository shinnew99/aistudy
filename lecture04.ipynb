{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 준비\n",
    "이번 강의부터 실제 데이터에 대한 형태소 분석을 실시함에 따라 몇 가지 준비가 필요하다.\n",
    "\n",
    "## ujson 패키지 설치\n",
    "앞서 안내한 바와 같이 이 강좌에서는 자료의 표준 형식으로 JSON, 정확히는 JSON 라인 파일 형식을 사용한다. 이에 따라 고속의 JSON 처리를 위해 ujson(<https://pypi.python.org/pypi/ujson>) 모듈을 설치한다. 이 모듈은 아나콘다 파이썬의 패키지 저장소에서 관리되고 있으므로 아나콘다 파이썬의 표준 패키지 관리 도구인 Conda(<https://conda.io>)를 이용하여 설치할 수 있다. \n",
    "\n",
    "Conda는 명령행 도구이므로 먼저 그림과 같이 명령 프롬프트를 실행하여 명령행 창을 연다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![메뉴에서 명령창 실행](figs/menu-cmd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이어서 `conda install ujson` 명령을 입력하고 [ENTER]를 누르면 다음과 같이 ujson 패키지, 그리고 업데이트 가능한 패키지들을 목록을 보여준다. 계속 진행할 것인가는 물음에 'Y'로 답하면 해당 패키지들을 내려받아 설치한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "C:\\Users\\leekh>conda install ujson\n",
    "Fetching package metadata .........\n",
    "Solving package specifications: ..........\n",
    "\n",
    "Package plan for installation in environment C:\\Users\\leekh\\Anaconda3:\n",
    "\n",
    "The following packages will be downloaded:\n",
    "\n",
    "    package                    |            build\n",
    "    ---------------------------|-----------------\n",
    "    conda-env-2.6.0            |                0          498 B\n",
    "    requests-2.13.0            |           py35_0         823 KB\n",
    "    ujson-1.35                 |           py35_0          21 KB\n",
    "    pyopenssl-16.2.0           |           py35_0          70 KB\n",
    "    conda-4.3.17               |           py35_0         563 KB\n",
    "    ------------------------------------------------------------\n",
    "                                           Total:         1.4 MB\n",
    "\n",
    "The following NEW packages will be INSTALLED:\n",
    "\n",
    "    conda-env: 2.6.0-0\n",
    "    ujson:     1.35-py35_0\n",
    "\n",
    "The following packages will be UPDATED:\n",
    "\n",
    "    conda:     4.2.9-py35_0  --> 4.3.17-py35_0\n",
    "    pyopenssl: 16.0.0-py35_0 --> 16.2.0-py35_0\n",
    "    requests:  2.11.1-py35_0 --> 2.13.0-py35_0\n",
    "\n",
    "Proceed ([y]/n)? y\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">파이썬에서 사용할 수 있는 대표적인 JSON 라이브러리 모듈들의 성능 비교 결과가 다음에 포스트에 실려있다. <http://blog.dataweave.in/post/87589606893/json-vs-simplejson-vs-ultrajson>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 자바 개발자 도구 설치\n",
    "이 강좌에서 모든 텍스트 처리는 파이썬 언어를 이용하여 이루어지지만 형태소 분석에는 자바 언어로 만들어진 도구를 사용한다. 그러므로 자바로 작성된 라이브러리를 원활히 사용할 수 있는 환경을 갖추어야 한다. 이를 위해 자바 개발 도구(Java Development Kit)를 설치한다.\n",
    "자바 개발 도구는 배포 사이트(<http://www.oracle.com/technetwork/java/javase/downloads/index.html>)에서 구할 수 있다. 표준판(SE: Standard Edition) 자바 개발자 도구 설치 프로그램을 내려받아 실행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![자바 개발 도구 표준판 내려받기](figs/jdk-download-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 자바 개발 도구 내려받기 그림을 누르면 운영체제와 플랫폼별로 준비된 설치 프로그램이 있는 곳으로 이동한다. 이곳에서 라이선스에 동의하고 자신의 환경에 맞는 설치 프로그램을 내려받아 실행하면 자바 개발 도구의 설치가 끝난다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![자바 개발 도구 설치 프로그램 내려받기](figs/jdk-download-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JPype 패키지 설치\n",
    "JPype(<https://pypi.python.org/pypi/JPype1>)는 파이썬에서 자바로 만들어진 라이브러리를 마치 파이썬으로 작성된 라이브러리처럼 사용할 수 있도록 해주는 모듈이다. 이 모듈을 윈도우에서 편리하게 설치하려면 비공식 윈도우 바이너리 파이썬 라이브러리 배포 사이트(Unofficial Windows Binaries for Python Extension Packages)인 <http://www.lfd.uci.edu/~gohlke/pythonlibs>에 접속하여  `.whl` 형식 패키지를 내려받아야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![비공식 윈도우 바이너리 파이썬 패키지 사이트의 JPype](figs/jpype-whl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대부분의 사용자들이 64비트 아키텍처 컴퓨터를 사용할 것이므로 `Jpype1-0.6.2-cp35-cp35m-win_amd64.whl` 파일을 내려받아 적절한 디렉토리, 예를 들어 사용자 홈디렉토리 밑의 `다운로드` 디렉토리(`C:\\Users\\leekh\\Downloads`)에 저장한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![탐색기로 명령 창 열기](figs/downloads-context.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 파이썬의 표준 패키지 관리 도구인 pip(<https://pypi.python.org/pypi/pip>)을 이용하여 내려받은 패키지를 설치해야 한다. 그런데 pip은 명령행 도구이므로 명령 창을 열어야 한다. 이 때 위의 화면과 같이 탐색기를 실행한 다음 명령 창을 열려는 디렉토리를 마우스 포인터로 가리킨 상태에서 시프트 키를 누르고 마우스 오른쪽 버튼을 눌러 표시된 컨텍스트 메뉴에서 `여기서 명령 창 열기`를 선택하는 것이 가장 편리하다. 이제 다음과 같이 `pip` 명령으로 내려받은 패키지를 설치한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pip으로 JPype 패키지 설치](figs/pip-jpype.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KoNLPy 패키지 설치\n",
    "이제 자바로 만들어진 형태소 분석 라이브러리를 JPype 모듈을 이용하여 파이썬에서 쉽게 쓸 수 있도록 해주는 라이브러리인 KoNLPy(<http://konlpy.org>)를 설치한다. 이 라이브러리 모듈도 명령행 창에서 `pip`으로 설치한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pip으로 KoNLPy 설치](figs/pip-konlpy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSVC 런타임 설치\n",
    "윈도우 환경에서 `msvcr100.dll` 파일이 없다는 메시지가 표시되면서 KoNLPy 모듈이 동작하지 않을 경우 <https://www.microsoft.com/ko-KR/download/details.aspx?id=14632>에서 **Microsoft Visual C++ 2010 재배포 가능 패키지(x64)**를 내려받아 설치해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Microsoft Visual C++ 2010 재배포 가능 패키지](figs/msvcrt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참고: 파이썬의 모듈, 패키지, 라이브러리\n",
    "### 개념과 용어\n",
    "파이썬 시계에서 모듈, 패키지, 라이브러리라는 용어들이 혼용되고 있는데 그 의미를 구분해 보면 다음과 같다.\n",
    "\n",
    "* **모듈**: 파이썬 튜토리얼 문서에 따르면 모듈이란 정의와 문을 담고 있는 파이썬 파일(a file containing Python definitions and statements)을 의미한다. 보통의 스크립트와 다른 점이라면 실행이 이루어지는 부분이 없다는 것이다. 그러므로 모듈은 다른 실행 스크립트에서 임포트되어 사용된다. 다른 모듈은 임포트하여 실행을 담당하는 모듈은 메인 모듈이라 부른다.\n",
    "* **패키지**: 파이썬에 있어서 패키지는 여러 개의 모듈들을 결합하여 구조화한 것을 말한다. 즉, 패키지는 여러 개의 `.py` 파일로 구성되며 특별한 규칙에 의해 조합된다. 한편, 패키지는 일반적인 표현으로는 배포의 단위를 나타내는 개념으로 보통 하나의 파일로 조성된다.\n",
    "* **라이브러리**: 라이브러리는 매우 폭이 넓은 일반적 표현으로 모듈, 패키지 등을 뭉뚱그려 가리킬 수 있으며, 특히 기능적 단위를 강조하는 표현이다.\n",
    "\n",
    "### 배포 방법에 따른 분류\n",
    "파이썬 모듈과 패키지들은 배포 방법에 따라 다음과 같이 나누어 볼 수 있다.\n",
    "\n",
    "* **표준 라이브러리**: 파이썬 표준 배포판에 기본으로 포함된 상태로 배포되는 라이브러리들을 표준 라이브러리라 한다. 새로이 표준 라이브러리로 선정된 라이브러리를 이전 버전 파이썬에 추가하기도 하는데 이를 백포트(backports)라 부른다.\n",
    "* **제3자 라이브러리**: IT 세계에서 제3자(third party)란 제작사(판매자)와 고객(구매자)이 아닌 공급자를 말한다. 그러므로 파이선에 제3자 라이브러리라 함은 표준 라이브러리가 아니며, 사용자 직접 만들지 않은 라이브러리들을 통칭하는 표현이다. 파이썬 세계에서는 제3자 라이브러리의 효율적인 배포를 위해 파이썬 패키지 인덱스(PyPI, <https://pypi.python.org>)를 이용한다. 이 사이트를 통해 배포되는 패키지들은 표준 파이선 패키지 관리 도구인 `pip`을 이용하여 설치와 업그레이드가 가능하다.\n",
    "\n",
    "한편 아나콘다 파이썬 등의 비표준 배포판들은 배포판의 목적에 부합하는 제3자 라이브러리들을 배포판에 기본적으로 포함하여 배포한다. 예를 들어 아나콘다 파이썬은 requests, BeautifulSoup 등을 표준 라이브러리에 더하여 기본적으로 포함하고 있다. 또한 비표준 배포판들은 선별된 제3자 라이브러리들을 파이썬 패키지 인덱스가 아닌 자체 저장소를 통하여 관리하기도 한다. 예를 들어, 아나콘다 파이썬은 ujson 모듈을 자체 저장소에서 배포하며 이는 아나콘다 파이썬의 표준 패키지 관리 도구인 `conda`를 이용해 설치와 업그레이드를 할 수 있다.\n",
    "\n",
    "### 모듈 파일 형식에 따른 분류\n",
    "파이썬 패키지는 복수의 `.py` 파일로 구성되므로 배포를 위해서는 여러 개의 파일을 하나의 파일로 묶을 수 있는 압축 파일 형식을 도입해야 한다. 기존에는 에그(egg) 형식 등이 사용되었는데 최근에는 휠(wheel) 형식이 표준으로 자리잡고 있다. 그런데 중요한 것은 이러한 외적 형식이 아니라 이 외적 형식에 포함된 모듈 파일 형식의 종류이다.\n",
    "\n",
    "* **소스 패키지**: 소스 패키지란 모듈 파일이 파이썬 소스인 `.py` 파일, 혹은 파이썬과 쉽게 결합되는 C/C++ 소스로 구성된 패키지이다. 파이썬 소스로만 구성된 패키지는 플랫폼 의존성이 없고 쉽게 설치할 수 있다.\n",
    "* **바이너리 패키지**: C/C++로 제작된 라이브러리의 경우 설치할 때에 C/C++ 컴파일러가 필요하다. 모든 사용자의 환경에 컴파일러가 설치되지 않을 수도 있고 의존성 문제가 발생할 수도 있기 때문에 플랫폼별로 따로따로 컴파일하여 바이너리 패키지를 조성하여 배포하는 것이 권장된다. 바이너리 패키지에도 여러 가지 형식이 있을 수 있는데 최근에 권장되는 형식은 wheel 형식으로 패키지 파일의 확장자가 `.whl`이다.\n",
    "\n",
    "패키지 파일을 조성하여 파이썬 패키지 인덱스에 업로드하는 일은 패키지 제작자가 감당하여야 하는 일인데, 바이너리 패키지의 조성이 원활하지 않아 제공되지 않는 경우가 있다. C/C++ 컴파일러를 비교적 쉽게 설치할 수 있는 리눅스나 맥오에스 환경에서는 C/C++ 소스가 포함된 소스 패키지만 제공되어도 큰 문제가 없지만 윈도우의 경우에는 설치가 매우 어려울 수 있다. 이러한 이유로 비공식 윈도우 바이너리 파이썬 라이브러리 배포 사이트(Unofficial Windows Binaries for Python Extension Packages)인 <http://www.lfd.uci.edu/~gohlke/pythonlibs>가 존재하게 된 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 형태소와 형태소 분석의 개념\n",
    "형태소란 언어에 있어서 \"최소 의미 단위\"를 말한다. 이 때 의미는 어휘적 의미와 문법적 의미를 모두 포함한다. 형태소 분석이란 형태소보다 단위가 큰 언어 단위인 어절, 혹은 문장을 최소 의미 단위인 형태소로 분절하는 과정이다. 오해하지 말아야 할 것은 형태소**를** 분석하는 것이 아니라 형태소**로** 분석하는 것이라는 사실이다. \"어절 분석\" 혹은 \"형태론적 분석\"이 더 적합한 용어라고 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">그런 의미에서 영어 용어 morphological analysis가 더 적합하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "형태소 분석의 실제 예를 살펴보자. 다음은  \"나는 배가 아파서 걸어서 집에 갔습니다\"라는 문장을 형태소 분석한 결과이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "나/대명사 는/보조사 배/명사+가/격조사 아프/형용사+아서/연결어미 걷/동사+어서/연결어미 \n",
    "집/명사+에/격조사 가/동사+았/선어말어미+습니다/어말어미\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 `나`, `배`, `아프`, `걷`, `집`, `가`는 어휘적 의미를 나타냈고 있으며 이들을 어휘 형태소라고 부른다. 한편, `는`, `가`, `아서`, `어서`, `에`, `았`, `습니다`는 문법적 의미를 나타내며 이러한 형태소들을 문법 형태소라고 부른다.\n",
    "\n",
    "위의 형태소 분리 기준은 표준국어문법(남기심, 고영근 (2014) 『표준국어문법론』. 제4판. 탑출판사.)에 따른 것이다. 중세국어의 지식을 지닌 사람은 어말어미 `습니다`는 최소 의미 단위로의 분석이 덜 되었다고 주장할 수도 있다. 즉, `습`+`니`+`다`가 맞다고 할 수도 있다. 이러한 논의는 국어학에서의 논의이기에 우리는 현대어의 기술 문법인 표준국어문법을 따르기로 하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "형태소 분석은 간단해 보이지만 사실은 매우 복잡한 과정이다. 많은 경우에 명사와 조사의 분리, 동사와 형용사의 어간과 어미의 분리는 각각의 이들 어휘의 사전만 있으면 가능할 것으로 보이지만 불규칙 용언의 활용 등을 처리하려면 복잡한 규칙이 필요하다. 또한 형태소 분석은 항상 정답을 결정하기 어려운 경우도 있다. 그러므로 성능이 좋은 형태소 분석기의 구현은 쉽지 않은 문제이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">좋은 소식은 한국어 형태소 분석 기술이 성숙 단계에 이르러서 누구나 쓸 수 있는 공개 소프트웨어 형태소 분석기가 꽤 여러 개 존재한다는 것이다. 다만, 상업적으로 이용할 때에는 제한이 있을 수 있으니 형태소 분석기별로 사용자 라이선스를 잘 확인해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "나는 -> 나/대명사+는/보조사 | 나/동사+는/관형형어미 | 날/동사+는/관형형어미\n",
    "걸어서 -> 걷/동사+어서/연결어미 | 걸/형용사+어서/연결어미 | 걸/동사+어서/연결어미\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "에서 보는 것과 같이 형태소 분석의 기본 입력인 하나의 어절에 대하여 여러 개의 형태소 분석 결과가 있을 수 있기 때문이다. 그러므로 실용적인 형태소 분석기는 이와 같은 형태소 분석의 중의성을 해결하는 과정이 필요하다. 이 과정을 품사 태깅(part-of-speech tagging), 혹은 줄여서 태깅이라고 부른다. 정리하면 좁은 의미의 형태소 분석은 하나의 어절에 대하여 가능한 모든 형태소 분석 결과를 출력하는 절차이며, 넓은 의미의 형태소 분석은 문맥을 고려하여 하나의 어절에 대하여 가장 적합한 것으로 판단되는 하나의 형태소 분석 결과만을 출력하는 품사 태깅을 포함한 절차이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">사실 품사 태깅이라는 용어는 한국어의 형태소 분석에 있어서 아주 정확한 용어는 아니다. 품사는 그 정의상 단어에만 부여할 수 있는 것이다. 표준국어문법에서 조사는 단어이지만 어미는 단어가 아니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 형태소 분석 라이브러리 사용하기\n",
    "## 형태소 분석 라이브러리 동작 실험\n",
    "앞서 설치한 형태소 분석 라이브러리가 제대로 동작하는지 확인하기 위해 다음 스크립트를 실행해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'konlpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-0b2f789dacbb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkonlpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHannanum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'konlpy'"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Hannanum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'konlpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-7698c1133c1e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkonlpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHannanum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mhannanum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHannanum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"분석할 텍스트를 입력하세요: \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mma_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhannanum\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'konlpy'"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Hannanum\n",
    "\n",
    "hannanum = Hannanum()\n",
    "text = input(\"분석할 텍스트를 입력하세요: \")\n",
    "ma_result = hannanum.pos(text)\n",
    "\n",
    "print(ma_result)\n",
    "\n",
    "for lex, pos in ma_result:\n",
    "    print(\"{}\\t{}\".format(lex, pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KoNLPy는 현재 다섯 종류의 형태소 분석 라이브러리를 지원한다. 이 가운데 윈도우에서 이용할 수 있는 라이브러리들은 자바로 구현된 Kkma, Komoran, Hannanum과 스칼라로 구현된 Twitter이다. C++로 구현된 Mecab은 윈도우에서 지원하지 않는다.\n",
    "\n",
    "이들 라이브러리들은 실행 속도, 정확도, 사용자 사전 용이성 등의 특성이 모두 다르다. 실행 속도와 정확도에 대한 간략한 비교는 <http://konlpy.org/en/v0.4.4/morph/#comparison-between-pos-tagging-classes>에서 볼 수 있다. 이 강좌에서는 Hannanum을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">이 강좌에서 Hannanum을 이용하는 것은 사용자 정의 사전의 작성과 이용이 간단하기 때문이다. Hannanum에 대한 상세한 정보는 공식 페이지(<http://semanticweb.kaist.ac.kr/home/index.php/HanNanum>)에서 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hannanum에서 지원하는 형태소 분석 메소드 몇 가지를 더 실험해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'konlpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-faf3495ca667>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkonlpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHannanum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mhannanum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHannanum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"어느새 뜨거운 여름이 되어 빙수가 생각난다.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhannanum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmorphs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'konlpy'"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Hannanum\n",
    "\n",
    "hannanum = Hannanum()\n",
    "text = \"어느새 뜨거운 여름이 되어 빙수가 생각난다.\"\n",
    "print(hannanum.morphs(text))\n",
    "print(hannanum.nouns(text))\n",
    "print(hannanum.pos(text, ntags=22))\n",
    "print(hannanum.analyze(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 이미 사용한 바 있는 `pos()` 메소드에 `ntags=22`로 키워드 인자를 지정하면 상세 품사가 부착된 형태소 분석 결과를 얻을 수 있다. 또한 `analyze()` 메소드는 형태소 분석 중의성이 해결되기 이전의 모든 형태소 분석 결과를 보여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 품사 집합\n",
    "`pos()` 메소드가 돌려주는 형태소 분석 결과는 형태소와 품사로 이루어진 튜플의 리스트이다. 이 형태소 분석 결과를 이용하기 위해서는 형태소 분석기가 돌려주는 품사 집합(part-of-speech tag set)를 알고 있어야 한다. 이 강좌에서 우리가 사용할 `Hannanum` 클래스의 품사 집합은 다음과 같다. 먼저 총 9개의 품사 체계인 단순 품사 집합은 아래 표와 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 태그 | 품사   |\n",
    "|:----:|:-------|\n",
    "|   N  | 체언   |\n",
    "|   P  | 용언   |\n",
    "|   M  | 수식언 |\n",
    "|   I  | 독립언 |\n",
    "|   J  | 관계언 |\n",
    "|   E  | 어미   |\n",
    "|   X  | 접사   |\n",
    "|   S  | 기호   |\n",
    "|   F  | 외국어 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경우에 따라서는 위보다 더 자세한 품사 집합이 필요하기도 하다. 그럴 때에는 22개의 품사로 이루어진 아래의 상세 품사 집합을 이용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 태그 | 품사        |\n",
    "|:----:|:------------|\n",
    "|  NC  | 보통 명사   |\n",
    "|  NQ  | 고유 명사   |\n",
    "|  NB  | 의존 명사   |\n",
    "|  NN  | 수사        |\n",
    "|  NP  | 대명사      |\n",
    "|  PV  | 동사        |\n",
    "|  PA  | 형용사      |\n",
    "|  PX  | 보조 용언   |\n",
    "|  MM  | 관형사      |\n",
    "|  MA  | 부사        |\n",
    "|  II  | 감탄사      |\n",
    "|  JC  | 격조사      |\n",
    "|  JX  | 보조사      |\n",
    "|  JP  | 서술격 조사 |\n",
    "|  EP  | 선어말 어미 |\n",
    "|  EF  | 종결 어미   |\n",
    "|  EC  | 연결 어미   |\n",
    "|  ET  | 전성 어미   |\n",
    "|  XP  | 접두사      |\n",
    "|  XS  | 접미사      |\n",
    "|  S   | 기호        |\n",
    "|  F   | 외국어      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 형태소 분석의 입출력 설계\n",
    "이미 밝힌 대로 이 강좌에서는 분석 대상 자료를 JSON 라인 형식으로 저장한다. 그러므로 형태소 분석 대상인 문자열, 예를 들어 신문 기사 본문은 JSON의 특정 키에 대당하는 값으로 주어질 것이다. 또한 이 강좌의 텍스트 처리는 오프라인 일괄 처리를 전제하므로 형태소 분석을 일괄적으로 수행하여 저장한다. 따라서  형태소 분석 결과 역시 JSON 형식으로 저장되어야 할 것이다.\n",
    "\n",
    "### 입력\n",
    "`pos()` 메소드의 입력으로는 제법 긴 문자열을 지정할 수 있다. 그러나 길이가 제법 길어질 수 있는 신문 기사 본문 한 건을 모두 `pos()` 메소드의 인자로 주는 것은 효과적이지 못하다. 따라서 입력 텍스트의 적절한 분절이 필요하다. 여러모로 가장 적절한 분절은 문장이다.\n",
    "\n",
    "문장은 텍스트 파일의 내재적 단위가 아니다. 즉, 문장과 문장을 구분하는 명시적인 구분자가 존재하지 않는다. 텍스트 처리에서는 문장을 문장의 종결을 나타내는 문장 부호인 ., ?, !로 구분하는 방법을 많이 사용한다. 그런데 이들 문장 부호가 때로 문장의 종결이 아닌 곳에서도 사용될 수 있기 때문에 이들 부호에 공백 문자가 연이어진 경우를 문장의 구분이 이루어지는 것으로 보는 것이 안전하다. 실제 구현에 있어서는 문장의 구분을 줄의 구분과 일치시켜서 문장의 구분이 텍스트 파일의 외현적 구조에 반영되도록 하는 것이 편리하다.\n",
    "\n",
    "다음은 위의 생각을 코드로 구현한 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 분절 실험\n",
    "\n",
    "text = \"파이썬은 데이터 분석에 적절한 언어이다. \" + \\\n",
    "    \"하지만 프로그램의 실행 속도가 빠르지 않다(?)는 평가가 있지 않은가! \" + \\\n",
    "    \"실행 속도는 언어 선택에 있어서 중요한 요소이긴 하지만 항상 그런가?\\n\\n\\n\"\n",
    "    \n",
    "text = text.strip()\n",
    "text = text.replace(\". \", \".\\n\")\n",
    "text = text.replace(\"? \", \"?\\n\")\n",
    "text = text.replace(\"! \", \"!\\n\")\n",
    "sentences = text.splitlines()\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "물론 위의 코드는 문장이 중간에서 줄바꿈 문자로 구분되어 두 개 이상의 줄로 나뉘지 않는다고 전제한 것이다. 이 전제가 만족되지 않을 경우에는 줄바꿈 문자를 보통의 공백 문자로 바꾸어 문장 분절을 수행해야 한다. 위 코드를 다음과 같이 재사용 가능한 사용자 함수로 만들어 이용하편 편리하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 문장 분절 함수\n",
    "\n",
    "def split_sentences(text):\n",
    "    text = text.strip()\n",
    "    text = \\\n",
    "        text.replace(\". \", \".\\n\").replace(\"? \", \"?\\n\").replace(\"! \", \"!\\n\")\n",
    "    sentences = text.splitlines()\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "\n",
    "text = \"파이썬은 데이터 분석에 적절한 언어이다. \" + \\\n",
    "    \"하지만 프로그램의 실행 속도가 빠르지 않다(?)는 평가가 있지 않은가! \" + \\\n",
    "    \"실행 속도는 언어 선택에 있어서 중요한 요소이긴 하지만 항상 그런가?\\n\\n\\n\"\n",
    "    \n",
    "sentences = split_sentences(text)\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 문장 분절 함수는 의도한 대로 동작한다. 그런데 위의 함수는 효율적이지는 않다. 동일한 문자열에 대해서 문자열 치환을 세 번을 반복하기 때문이다. 이를 개선하려면 문자열 치환을 한 번에 수행해야 하는데, 이는 정규 표현으로 해결할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 정규 표현을 이용한 문장 분절 함수(sub() 함수 사용)\n",
    "\n",
    "import re\n",
    "\n",
    "def split_sentences(text):\n",
    "    text = text.strip()\n",
    "    text = re.sub(\"([!?.]) \", \"\\g<1>\\n\", text)\n",
    "    sentences = text.splitlines()\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "\n",
    "text = \"파이썬은 데이터 분석에 적절한 언어이다. \" + \\\n",
    "    \"하지만 프로그램의 실행 속도가 빠르지 않다(?)는 평가가 있지 않은가! \" + \\\n",
    "    \"실행 속도는 언어 선택에 있어서 중요한 요소이긴 하지만 항상 그런가?\\n\\n\\n\"\n",
    "    \n",
    "sentences = split_sentences(text)\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 사용된 정규 표현은 '.', '?', '!' 가운데 한 문자와 공백 문자가 연접한 문자열이 입력 문자열에서 일치했을 때 일치한 문장 부호와 줄바꿈 문자가 연접한 문자열로 치환되도록 한다. 그룹 매치와 후방 참조(back reference)를 이용한 것이다. 사실 위의 코드는 더 개선될 여지가 있다. re 모듈에서 제공하는 문자열 분리 함수인 `split()` 함수를 이용하면 치환 과정을 거치지 않고 바로 문장 분리를 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 정규 표현을 이용한 문장 분절 함수(split() 함수 사용)\n",
    "\n",
    "import re\n",
    "\n",
    "def split_sentences(text):\n",
    "    sentences = re.split(\"(?<=[.?!]) \", text.strip())\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "\n",
    "text = \"파이썬은 데이터 분석에 적절한 언어이다. \" + \\\n",
    "    \"하지만 프로그램의 실행 속도가 빠르지 않다(?)는 평가가 있지 않은가! \" + \\\n",
    "    \"실행 속도는 언어 선택에 있어서 중요한 요소이긴 하지만 항상 그런가?\\n\\n\\n\"\n",
    "    \n",
    "sentences = split_sentences(text)\n",
    "\n",
    "for sentence in sentences:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 사용된 정규 표현은 생김새가 복잡한데, `split()` 함수의 의미는 '.', '?', '!' 가운데 한 문자가 선행하는 공백 문자를 기준으로 입력 문자열을 분절하라는 뜻이다.\n",
    "\n",
    "### 출력\n",
    "\n",
    "형태소 분석 결과를 응용 프로그램에서 바로 사용할 경우에는 리스트로 주어지는 형태소 분석 결과를 그대로 쓰면 된다. 그런데 형태소 분석 결과를 파일에 저장하였다가 재사용하려는 경우에는 몇 가지 고려해야 할 사항이 있다.\n",
    "\n",
    "첫 번째는 입력 텍스트의 구조, 즉 단락 구분이나 문장 구분, 그리고 어절 구분 등을 얼마나 반영할 것인가의 고려이다. 이는 텍스트 분석의 목표와 과제에 따라 결정된다. 이 강좌에서는 단락의 구분은 하지 않으며, 문장 구분은 유지하기로 하자. 어절의 구분은 그 방법과 형태를 알아 보기는 하되 실제로 반영하지는 않는다.\n",
    "\n",
    "두 번째는 저장 형식의 선택이다. 형태소 분석 결과는 앞에서 논의한 것처럼 일정 수준의 구조적 정보를 담고 있으므로 재사용의 편의를 위하여 구조가 반영된 형태로 저장하는 것이 좋다. 이 강좌에서는 JSON 형식을 이용한다. 예를 들면 아래과 같이 하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "{\n",
    "    text = \"어느새 여름이 성큼 다가왔습니다.\",\n",
    "    ma_res = [\n",
    "        [\n",
    "            [\"어느새\", \"MA\"],\n",
    "            [\"여름\", \"NC\"],\n",
    "            [\"이\", \"JC\"],\n",
    "            [\"성큼\", \"MA\"],\n",
    "            [\"다가오\", \"PV\"],\n",
    "            [\"아\", \"EP\"],\n",
    "            [\"ㅂ니다\", \"EF\"],\n",
    "            [\".\", \"SF\"]\n",
    "        ]\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 형태소 분석 결과를 저장하는 리스트 `ma_res` 안에 리스트가 한 겹 더 있는 것은 입력 텍스트가 여러 개의 문장으로 분절될 경우 문장별 형태소 분석 결과를 분리하여 저장하기 위해서이다. \n",
    "즉, `ma_res`는 입력 텍스트의 문장별 형태소 분석 결과를 원소로 하는 리스트이다.\n",
    "\n",
    "앞서 언급한 바와 같이 경우에 따라서는 개별 형태소와 문장 사이에 존재하는 어절 층위를 반영하는 것이 필요할 수도 있다. 그렇게 하면 아래와 같이 리스트의 층위가 한 겹 더 생긴다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "{\n",
    "    text = \"어느새 여름이 성큼 다가왔습니다.\",\n",
    "    ma_res = [\n",
    "        [\n",
    "            [\n",
    "                [\"어느새\", \"MA\"]\n",
    "            ],\n",
    "            [\n",
    "                [\"여름\", \"NC\"],\n",
    "                [\"이\", \"JC\"]\n",
    "            ],\n",
    "            [\n",
    "                [\"성큼\", \"MA\"],\n",
    "            ],\n",
    "            [\n",
    "                [\"다가오\", \"PV\"],\n",
    "                [\"아\", \"EP\"],\n",
    "                [\"ㅂ니다\", \"EF\"],\n",
    "                [\".\", \"SF\"]\n",
    "            ]\n",
    "        ]\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "물론 단락, 장절 등 문장보다 단위가 큰 언어 단위 구분도 필요에 따라 도입할 수 있을 것이다. 이 강좌에서는 문장과 형태소의 구분만 유지하는 JSON 형식을 사용한다.\n",
    "\n",
    "### 참고\n",
    "전산 언어학이나 말뭉치 언어학에서는 다음과 같은 어절 단위 수직 텍스트 형식이나 문장 단위 수평 형식도 많이 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# 어절 단위 수직 텍스트 형식\n",
    "어느새 어느새/MA\n",
    "가을이 가을/NC+이/JC\n",
    "성큼  성금/MA\n",
    "다가왔습니다. 다가오/PV+았/EP+습니다/EF+./SF\n",
    "\n",
    "# 문장 단위 수평 형식\n",
    "어느새_어느새/MA 가을이_가을/NC+이/JC 성큼_성큼/MA 다가왔습니다._다가오/PV+았/EP+습니다/EF+./SF\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또한 XML에 기반한 주석 말뭉치 형식의 국제 표준인 XCES(<http://www.xces.org>)도 있다.\n",
    "\n",
    "### 형태소 분석 함수의 구현\n",
    "앞서 구현한 문장 분절 함수 `split_sentences()`와 형태소 분석 클래스의 `pos()` 메소드를 결합하여 문장 단위의 형태소 분석 결과를 리스트로 돌려주는 함수를 구현하면 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 형태소 분석 함수\n",
    "\n",
    "import re\n",
    "import ujson\n",
    "from konlpy.tag import Hannanum\n",
    "\n",
    "\n",
    "def split_sentences(text):\n",
    "    all_sentences = []\n",
    "    lines = text.strip().splitlines()\n",
    "    \n",
    "    for line in lines:\n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "        sentences = re.split(\"(?<=[.?!]) \", line)\n",
    "        all_sentences += sentences\n",
    "    \n",
    "    return all_sentences\n",
    "\n",
    "\n",
    "def get_morph_anal(text):\n",
    "    hannanum = Hannanum()\n",
    "    sent_morph_anals = []\n",
    "    sentences = split_sentences(text)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sent_morph_anal = hannanum.pos(sentence, ntags=22)\n",
    "        sent_morph_anals.append(sent_morph_anal)\n",
    "        \n",
    "    return sent_morph_anals\n",
    "\n",
    "\n",
    "def main():\n",
    "    my_text = \"\"\"<벌레이야기>\n",
    "\n",
    "1\n",
    "아내는 알암이의 돌연스런 가출이 유괴에 의한 실종으로 확실시되고 난 다음에도 한동안은 악착스럽게 자신을 잘 견뎌 나가고 있었다. 그것은 아이가 어쩌면 행여 무사히 되돌아오게 될지도 모른다는 간절한 희망과, 녀석에게 마지막 불행한 일이 생기기 전에 어떻게든지 놈을 다시 찾아내고 말겠다는 어미로서의 강인한 의지와 기원 때문인 것 같았다.\n",
    "지난해 5월 초. 어느 날 알암이가 학교에서 돌아올 시각이 훨씬 지나도록 귀가를 안 했다.\n",
    "\"\"\"\n",
    "\n",
    "    ma_res = get_morph_anal(my_text)\n",
    "    print(ma_res)\n",
    "    \n",
    "    \n",
    "# 실행\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 `split_sentence()` 함수는 기본적으로 앞서 실험한 것과 같지만 입력 텍스트에 존재하는 기본적인 줄 구분을 유지하도록 하였다. 이 때 한 문장이 여러 줄로 나뉘는 경우는 없다고 전제하고 한 줄 안에 여러 개의 문장이 있을 수 있는 것으로 간주하고 문장 분절을 수행한다.  이 때 줄바꿈 문자가 두 번 이어져서 분절되는 빈 문장은 제거한다.\n",
    "\n",
    "`get_morph_anal()` 함수는 형태소 분석기 객체와 분석 대상 텍스트를 인자로 받아 문장 문절을 수행한 뒤 형태소 분석 결과를 문장별로 수행하여 이를 리스트로 만들어 돌려준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 형태소 분석 후처리와 사용자 사전의 작성\n",
    "## 파생어 분석의 후처리\n",
    "Hannanum 형태소 분석기는 `강렬하면서도 다채롭다`이라는 텍스트를 다음과 같이 분석한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[('강렬', 'NC'),\n",
    " ('하', 'XS'),\n",
    " ('면서', 'EC'),\n",
    " ('도', 'JX'),\n",
    " ('다채', 'NC'),\n",
    " ('롭', 'XS'),\n",
    " ('다', 'EF')]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "즉, 형용사인 `강렬하다`와 `다채롭다`를 파생 단계까지 분석한다. 텍스트 분석의 목적에 따라 이러한 세부 분석이 적합할 때도 있지만 경우에 따라서는 다음의 분석이 더 적합할 수도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "[('강렬하', 'PA'),\n",
    " ('면서', 'EC'),\n",
    " ('도', 'JX'),\n",
    " ('다채롭', 'PA'),\n",
    " ('다', 'EF')]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러므로 경우에 따라서는 형태소 분석 결과에서 파생어 분석 처리를 어휘 단계로 후처리를 해야 한다. 접두사(XP)에 의한 파생어에 대해서도 비슷한 처리가 필요할 수 있다. 어떠한 처리를 해야 할지는 대규모 말뭉치의 형태소 분석 결과를 검토하여 결정해야 할 것이다.\n",
    "\n",
    "형태소 분석 결과의 후처리에는 부분 리스트를 치환하는 방법을 적용할 수 있다. 다음 소스를 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 부분 리스트 치환에 의한 형태소 분석 결과 후처리\n",
    "import itertools\n",
    "\n",
    "\n",
    "def find_sublists(seq, sublist):\n",
    "    length = len(sublist)\n",
    "    for index, value in enumerate(seq):\n",
    "        if value == sublist[0] and seq[index:index+length] == sublist:\n",
    "            yield index, index+length\n",
    "            \n",
    "\n",
    "def replace_sublist(seq, target, replacement, maxreplace=None):\n",
    "    sublists = find_sublists(seq, target)\n",
    "    if maxreplace:\n",
    "        sublists = itertools.islice(sublists, maxreplace)\n",
    "    for start, end in sublists:\n",
    "        seq[start:end] = replacement\n",
    "        \n",
    "        \n",
    "post_proc_pairs = [\n",
    "    ([(\"강렬\", \"NC\"), (\"하\", \"XS\")], [(\"강렬하\", \"PA\")]),\n",
    "    ([(\"다채\", \"NC\"), (\"롭\", \"XS\")], [(\"다채롭\", \"PA\")])\n",
    "]\n",
    "        \n",
    "ma_res = [(\"강렬\", \"NC\"), (\"하\", \"XS\"), (\"면서\", \"EC\"), (\"도\", \"JX\"), \n",
    "          (\"다채\", \"NC\"), (\"롭\", \"XS\"), (\"다\", \"EF\")]\n",
    "\n",
    "for src, dst in post_proc_pairs:\n",
    "    replace_sublist(ma_res, src, dst)\n",
    "    \n",
    "print(ma_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 코드를 실행해 보면 파생어에 대한 후처리가 의도한 대로 동작하는 것을 확인할 수 있다. 위의 코드의 핵심은 이른바 생성자(generator)의 사용이다. 이는 입력으로 주어진 형태소 분셕 결과 리스트를 지속적으로 경신하면서 치환을 진행하는 데에 결정적인 역할을 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">위의 코드는 스택오버플로우에서 질문과 답변(<http://stackoverflow.com/questions/12898023/replacing-a-sublist-with-another-sublist-in-python>)으로 주어진 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 대표형의 처리\n",
    "형태소 분석기는 어휘 형태소의 대표형을 찾아준다. 그러나 조사나 어미에 대해서는 그러한 처리를 하지 않는다. 즉, `학교를`, `신발을`에 대하여"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "학교/NNG+를/JKO\n",
    "신발/NNG+을/JKO\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "로 분석한다. 이 때 `를`과 `을`은 의미와 기능이 같은 목적격 조사로서 앞에 오는 명사류의 말음이 자음인지 모음인지에 따라 선택되는 음운론적 변이형이다. 그러므로 경우에 따라 이들 가운데 하나를 대표형으로 설정하고 다른 변이형을 대표형으로 치환해 주는 후처리가 요청될 수도 있다.\n",
    "\n",
    "## 사용자 사전 작성\n",
    "형태소 분석기에서는 형태소 분석을 위해 형태소 사전을 이용한다. 형태소 사전에는 개별 형태소와 이들의 품사, 불규칙 정보, 형태소 간의 연결 가능성을 나타내는 접속 정보 등이 포함된다. 이 사전에 모든 형태소가 포함될 수는 없으므로 형태소 분석기들을 사용자 형태소 사전을 통해 어휘를 보충할 수 있도록 하는 경우가 많다. 또한 어절별로 형태소 분석 결과를 미리 지정하여 사용하는 기분석 사전을 지원하기도 한다.\n",
    "\n",
    "이 강좌에서 사용하는 Hannanum 형태소 분석기는 사용자 사전 작성을 잘 지원한다. 사용자 사전은 konlpy 패키지 디렉토리 하위의 Hannanum 형태소 분석기에서 사용하는 데이터 디렉토리, 예를 들어 `C:\\Users\\leekh\\Anaconda3\\Lib/site-packages\\konlpy\\java\\data\\kE`에 들어 있다. 이 디렉토리에 들어 있는 확장자가 `txt`인 파일들의 명칭과 용도는 다음과 같다.\n",
    "\n",
    "* `connections.txt`: 세부 품사들 사이의 접속 정보를 기술한 파일\n",
    "* `connections_not.txt`: 형태소들 사이의 비접속 정보를 기술한 파일\n",
    "* `dic_analyzed.txt`: 선택된 어절들에 대한 형태소 분석 결과를 기술한 파일(기분석 사전)\n",
    "* `dic_system.txt`: 형태소 분석기에서 사용하는 시스템 형태소 사전 파일\n",
    "* `dic_user.txt`: 형태소 분석기에서 사용하는 사용자 형태소 사전 파일\n",
    "* `tag_set.txt`: 형태소 분석기에서 사용하는 형태소 태그 집합을 기술한 파일\n",
    "\n",
    "위의 파일 중 사용자 사전은 `dic_user.txt` 파일로 시스템 사전인 `dic_system.txt`과 같은 형식으로 형태소와 품사를 기술하면 된다. 불규칙 활용 용언의 경우 활용 정보도 함께 기술하여야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">최근 정보화진흥원에서 Hannanum 형태소 분석기에서 사용할 수 있는 확장된 형태소 사전을 공개하였다. 배포 페이지(<https://kbig.kr/index.php?page=0&sv=title&sw=&q=knowledge/pds_&tgt=view&page=1&idx=16451&sw=&sv=title?q=node/16451>)에서 내려받을 수 있다. 제공되는 사전의 규모가 매우 크므로 유의해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 연습 문제\n",
    "1. CSV 형식의 뉴스 데이터 파일을 형태소 분석하여 JSON 라인 형식의 텍스트 파일을 생성하라.\n",
    "1. TSV 형식의 메타 정보 파일과 곡별 가사 텍스트 파일로 이루어진 KPOP 데이터 파일을 형태소 분석하여 JSON 라인 형식의 텍스트 파일을 생성하라."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_metadata": {
   "author": "이기황",
   "coursetitle": "텍스트마이닝캠프",
   "courseyear": "2017",
   "date": "2017.05.11",
   "title": "KoNLPy 패키지를 이용한 형태소 분석"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
